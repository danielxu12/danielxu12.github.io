[
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code with no space in the folder name.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "danl_proj_nba.html#salary-distribution-among-teams",
    "href": "danl_proj_nba.html#salary-distribution-among-teams",
    "title": "Data Analysis Project",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet’s start with the salary distribution among teams using seaborn for visualization. ​​\n\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n/var/folders/_m/d6jf0jhd2zzdfd5kzdhl_24w0000gn/T/ipykernel_79892/1671011424.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  nba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 16))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams.\nNotice that Portland Trail Blazers has the highest total salary followed by Golden State Warriors and Philadelphia 76ers, and Memphis Grizzlies has the lowest total salary."
  },
  {
    "objectID": "danl_proj_nba.html#player-age-distribution",
    "href": "danl_proj_nba.html#player-age-distribution",
    "title": "Data Analysis Project",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let’s explore the Player Age Distribution across the NBA. We’ll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ​​\n\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\n# nba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\n# nba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n/Users/bchoe/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players.\nNotice that the majority of players fall within an age range from 24 to 34. There are few players whose age is above 40."
  },
  {
    "objectID": "danl_proj_nba.html#position-wise-salary-insights",
    "href": "danl_proj_nba.html#position-wise-salary-insights",
    "title": "Data Analysis Project",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we’ll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let’s create a box plot to visualize the salary distribution for each position. ​​\n\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. PG-SG has the highest median salary."
  },
  {
    "objectID": "danl_proj_nba.html#top-10-highest-paid-players",
    "href": "danl_proj_nba.html#top-10-highest-paid-players",
    "title": "Data Analysis Project",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we’ll identify the Top 10 Highest Paid Players in the NBA. Let’s visualize this information.\n\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'PlayerName',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Insightful Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSettings\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nDaniel\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarwars\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYour Name\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-320-python-basic.html",
    "href": "danl-320-python-basic.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-320-python-basic.html#what-is-python",
    "href": "danl-320-python-basic.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "danl-320-python-basic.html#variables-and-data-types",
    "href": "danl-320-python-basic.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-320-python-basic.html#control-structures",
    "href": "danl-320-python-basic.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "danl-320-python-basic.html#functions",
    "href": "danl-320-python-basic.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "danl-320-python-basic.html#lists-and-dictionaries",
    "href": "danl-320-python-basic.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Xu",
    "section": "",
    "text": "I am a full-time student at SUNY Geneseo, majoring in Economics with a minor in Data Analytics within the AACSB-accredited School of Business. I am actively seeking a Data Analyst or Business Analyst internship to gain hands-on experience and explore my passion for these fields.\nMy peers and mentors often describe me as hardworking, organized, and possessing strong communication skills. I am eager to contribute these qualities to a dynamic team."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel Xu",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Economics | Aug 2021 - May 2025  Minor in Data Analytics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel Xu",
    "section": "Experience",
    "text": "Experience\nResident Assistant (RA) | ResLife at Geneseo | Aug 2023 - Current"
  },
  {
    "objectID": "posts/Ice Cream/HW_2_Jupyter_Notebook_Blogging.html",
    "href": "posts/Ice Cream/HW_2_Jupyter_Notebook_Blogging.html",
    "title": "Settings",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-30-e88d64471ed3&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 terms = assembler3.getInputCols()\n      2 coefs = model3.coefficients.toArray()[:len(terms)]\n      3 stdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n      4 \n      5 df_summary = pd.DataFrame({\n\nNameError: name 'assembler3' is not defined\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)\n\n\nDataFrame\n\ndf = pd.read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\n\ndf.columns\n\nIndex(['priceper1', 'flavor_descr', 'size1_descr', 'household_id',\n       'household_income', 'household_size', 'usecoup', 'couponper1', 'region',\n       'married', 'race', 'hispanic_origin', 'microwave', 'dishwasher', 'sfh',\n       'internet', 'tvcable'],\n      dtype='object')\n\n\n\ndf2 = df[['priceper1', 'household_income', 'usecoup', 'couponper1', 'household_size']]\n\n\ndf2\n\n\n  \n    \n\n\n\n\n\n\npriceper1\nhousehold_income\nusecoup\ncouponper1\nhousehold_size\n\n\n\n\n0\n3.41\n130000\nTrue\n0.5\n2\n\n\n1\n3.50\n130000\nFalse\n0.0\n2\n\n\n2\n3.50\n130000\nFalse\n0.0\n2\n\n\n3\n3.00\n70000\nFalse\n0.0\n1\n\n\n4\n3.99\n130000\nFalse\n0.0\n3\n\n\n...\n...\n...\n...\n...\n...\n\n\n21969\n3.34\n80000\nFalse\n0.0\n4\n\n\n21970\n1.99\n80000\nFalse\n0.0\n4\n\n\n21971\n4.99\n80000\nFalse\n0.0\n1\n\n\n21972\n3.50\n80000\nFalse\n0.0\n1\n\n\n21973\n3.50\n80000\nFalse\n0.0\n1\n\n\n\n\n21974 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncorr = df2.corr()\n\n\n# Set up the matplotlib figure size\nplt.figure(figsize=(8, 6))\n\n# Generate a heatmap in seaborn:\n# - 'corr' is the correlation matrix\n# - 'annot=True' enables annotations inside the squares with the correlation values\n# - 'cmap=\"coolwarm\"' assigns a color map from cool to warm (blue to red)\n# - 'fmt=\".2f\"' formats the annotations to two decimal places\n# - 'linewidths=.5' adds lines between each cell\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Title of the heatmap\nplt.title('Correlation Heatmap with Varied Correlations')\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\ndf2 = spark.createDataFrame(df2)\n\ndtrain, dtest = df2.randomSplit([0.67, 0.33], seed = 1234)\n\n\n# assembling predictors\n\nassembler_predictors = (\n    ['household_income', 'usecoup', 'couponper1', 'household_size']\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"priceper1\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)\n\n# makting regression table\nprint( regression_table(model_1, assembler_1) )\n\n+------------------+--------+------+------------+---------+--------------+--------------+\n| y: priceper1     |   Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+------------------+--------+------+------------+---------+--------------+--------------+\n| household_income | -0.000 | ***  |      0.024 |   0.000 |       -0.048 |        0.048 |\n| usecoup          | -0.381 | ***  |      0.014 |   0.000 |       -0.409 |       -0.353 |\n| couponper1       |  0.375 | ***  |      0.004 |   0.000 |        0.367 |        0.383 |\n| household_size   | -0.030 | ***  |      0.018 |   0.000 |       -0.067 |        0.006 |\n| Intercept        |  3.495 | ***  |      0.000 |         |        3.495 |        3.495 |\n-----------------------------------------------------------------------------------------\n| Observations     | 14,734 |      |            |         |              |              |\n| R²               |  0.052 |      |            |         |              |              |\n| RMSE             |  0.650 |      |            |         |              |              |\n+------------------+--------+------+------------+---------+--------------+--------------+\n\n\n#RMSE\n\n# Create a new column for squared error\ndtest_1 = dtest_1.withColumn(\"error_sq\", pow(col(\"priceper1\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_1.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.649\n\n\n\nprint(compare_rmse([dtest_1], \"priceper1\"))\n\n+------+-----------+\n|      |   Model 1 |\n+======+===========+\n| RMSE |     0.649 |\n+------+-----------+\n\n\n\n\nResidual Plot\n\nresidual_plot(dtest_1, \"priceper1\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Create a Pandas DataFrame from model3's summary information\nterms = assembler_1.getInputCols()\ncoefs = model_1.coefficients.toArray()[:len(terms)]\nstdErrs = model_1.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]