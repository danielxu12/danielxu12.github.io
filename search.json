[
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code with no space in the folder name.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/Housing_Markets_danl_320_hw4_Xu_Daniel_files/Housing_Markets_danl_320_hw4_Xu_Daniel.html",
    "href": "posts/Housing_Markets_danl_320_hw4_Xu_Daniel_files/Housing_Markets_danl_320_hw4_Xu_Daniel.html",
    "title": "Housing Markets",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .4f}\",\n            significance_stars(p_value),\n            f\"{std_error: .4f}\",\n            f\"{ci_lower: .4f}\",\n            f\"{ci_upper: .4f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "posts/Housing_Markets_danl_320_hw4_Xu_Daniel_files/Housing_Markets_danl_320_hw4_Xu_Daniel.html#udfs",
    "href": "posts/Housing_Markets_danl_320_hw4_Xu_Daniel_files/Housing_Markets_danl_320_hw4_Xu_Daniel.html#udfs",
    "title": "Housing Markets",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .4f}\",\n            significance_stars(p_value),\n            f\"{std_error: .4f}\",\n            f\"{ci_lower: .4f}\",\n            f\"{ci_upper: .4f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "posts/Final Project Code/danl_320_XU_DANIEL_project.ipynb.html",
    "href": "posts/Final Project Code/danl_320_XU_DANIEL_project.ipynb.html",
    "title": "DANL 320 Project",
    "section": "",
    "text": "Website: https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets\nGithub: https://danielxu12.github.io\n#Setting\n\n# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nfrom tabulate import tabulate  # for table summary\n\n# For basic libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm  # for lowess smoothing\n\n# `scikit-learn`\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score)\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfrom sklearn.preprocessing import scale # zero mean & one s.d.\nfrom sklearn.linear_model import LassoCV, lasso_path\nfrom sklearn.linear_model import RidgeCV, Ridge\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\n\n# PySpark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\nData Set-up\n\nimport numpy as np\n\n\nimport kagglehub\nimport pandas as pd\nimport os\n\n# Download the dataset\npath = kagglehub.dataset_download(\"henriqueyamahata/bank-marketing\")\n\n# List files in the dataset directory\nprint(\"Files downloaded to:\", path)\nprint(\"Available files:\", os.listdir(path))\n\nFiles downloaded to: /kaggle/input/bank-marketing\nAvailable files: ['bank-additional-names.txt', 'bank-additional-full.csv']\n\n\n\nfrom google.colab import files\nuploaded = files.upload()  # This lets you browse and upload the file\n\nimport pandas as pd\ndf_pd = pd.read_csv('bank-additional-full.csv', sep=';', quotechar='\"')\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving bank-additional-full.csv to bank-additional-full (1).csv\n\n\n\ndf_pd\n\n\n  \n    \n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\n...\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic.4y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n1\n57\nservices\nmarried\nhigh.school\nunknown\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n2\n37\nservices\nmarried\nhigh.school\nno\nyes\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n3\n40\nadmin.\nmarried\nbasic.6y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n4\n56\nservices\nmarried\nhigh.school\nno\nno\nyes\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\nno\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41183\n73\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n41184\n46\nblue-collar\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n41185\n56\nretired\nmarried\nuniversity.degree\nno\nyes\nno\ncellular\nnov\nfri\n...\n2\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n41186\n44\ntechnician\nmarried\nprofessional.course\nno\nno\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nyes\n\n\n41187\n74\nretired\nmarried\nprofessional.course\nno\nyes\nno\ncellular\nnov\nfri\n...\n3\n999\n1\nfailure\n-1.1\n94.767\n-50.8\n1.028\n4963.6\nno\n\n\n\n\n41188 rows × 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndf_pd.columns\n\nIndex(['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n       'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays',\n       'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n       'cons.conf.idx', 'euribor3m', 'nr.employed', 'y'],\n      dtype='object')\n\n\n\ndf_pd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 41188 entries, 0 to 41187\nData columns (total 21 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   age             41188 non-null  int64  \n 1   job             41188 non-null  object \n 2   marital         41188 non-null  object \n 3   education       41188 non-null  object \n 4   default         41188 non-null  object \n 5   housing         41188 non-null  object \n 6   loan            41188 non-null  object \n 7   contact         41188 non-null  object \n 8   month           41188 non-null  object \n 9   day_of_week     41188 non-null  object \n 10  duration        41188 non-null  int64  \n 11  campaign        41188 non-null  int64  \n 12  pdays           41188 non-null  int64  \n 13  previous        41188 non-null  int64  \n 14  poutcome        41188 non-null  object \n 15  emp.var.rate    41188 non-null  float64\n 16  cons.price.idx  41188 non-null  float64\n 17  cons.conf.idx   41188 non-null  float64\n 18  euribor3m       41188 non-null  float64\n 19  nr.employed     41188 non-null  float64\n 20  y               41188 non-null  object \ndtypes: float64(5), int64(5), object(11)\nmemory usage: 6.6+ MB\n\n\n\ndf_pd['job'] = df_pd['job'].str.replace(\".\", \"_\")\ndf_pd['marital'] = df_pd['marital'].str.replace(\".\", \"_\")\ndf_pd['education'] = df_pd['education'].str.replace(\".\", \"_\")\ndf_pd['default'] = df_pd['default'].str.replace(\".\", \"_\")\ndf_pd['housing'] = df_pd['housing'].str.replace(\".\", \"_\")\ndf_pd['loan'] = df_pd['loan'].str.replace(\".\", \"_\")\ndf_pd['contact'] = df_pd['contact'].str.replace(\".\", \"_\")\n\n\n\ndf_pd['y'].unique()\n\narray(['no', 'yes'], dtype=object)\n\n\n\ndf_pd['y'] = np.where(df_pd['y'] == 'yes', 1, 0)\n\n\ndf_pd['y'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\ny\n\n\n\n\n\n0\n36548\n\n\n1\n4640\n\n\n\n\ndtype: int64\n\n\n\ndf_pd = df_pd.query('loan != \"unknown\"') #this drops 990 rows in df_pd\ndf = spark.createDataFrame(df_pd)\n\n\ndf_pd['job'].value_counts()\n\n\ndf_pd['marital'].value_counts()\n\n\ndf_pd['education'].value_counts()\n\n\ndf_pd['default'].value_counts()\n\n\n# Select specific columns\nselected_columns = ['age', 'duration', 'campaign']\n\n# Print descriptive summary for selected columns\nprint(\"\\nDescriptive Summary (Selected Columns):\")\nprint(df_pd[selected_columns].describe())\n\n\nDescriptive Summary (Selected Columns):\n                age      duration      campaign\ncount  40198.000000  40198.000000  40198.000000\nmean      40.025847    258.484253      2.567765\nstd       10.422343    259.387105      2.765476\nmin       17.000000      0.000000      1.000000\n25%       32.000000    102.000000      1.000000\n50%       38.000000    180.000000      2.000000\n75%       47.000000    320.000000      3.000000\nmax       98.000000   4918.000000     43.000000\n\n\n\nimport matplotlib.pyplot as plt\n\n# Plot histogram for 'duration'\nplt.figure(figsize=(10, 5))\nplt.hist(df_pd['job'], bins=20, edgecolor='black', color='skyblue')\nplt.title('Histogram of job')\nplt.xlabel('job')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n# Plot histogram for 'y' (only if it's numeric or categorical with few values)\nplt.figure(figsize=(10, 5))\nif df_pd['y'].dtype == 'object' or len(df_pd['y'].unique()) &lt; 10:\n    df_pd['y'].value_counts().plot(kind='bar', color='lightgreen', edgecolor='black')\n    plt.title('Bar Plot of Y')\n    plt.xlabel('Y')\n    plt.ylabel('Count')\nelse:\n    plt.hist(df_pd['y'], bins=20, edgecolor='black', color='lightgreen')\n    plt.title('Histogram of Y')\n    plt.xlabel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)\n\n\n# Replace dots or other special characters in column names\ndef clean_column_names(df):\n    for col_name in df.columns:\n        clean_name = col_name.replace('.', '_')\n        df = df.withColumnRenamed(col_name, clean_name)\n    return df\n\ndtrain = clean_column_names(dtrain)\ndtest = clean_column_names(dtest)\n\n\ndummy_cols_job, ref_category_job = add_dummy_variables('job', 0)\ndummy_cols_marital, ref_category_marital = add_dummy_variables('marital', 0)\ndummy_cols_education, ref_category_education = add_dummy_variables('education', 0)\ndummy_cols_default, ref_category_default = add_dummy_variables('default', 0)\ndummy_cols_housing, ref_category_housing = add_dummy_variables('housing', 0)\ndummy_cols_loan, ref_category_loan = add_dummy_variables('loan', 0)\n\nReference category (dummy omitted): admin_\nReference category (dummy omitted): divorced\nReference category (dummy omitted): basic_4y\nReference category (dummy omitted): no\nReference category (dummy omitted): no\nReference category (dummy omitted): no\n\n\n\ndtrain_pd = dtrain.toPandas()\n\n\n# assembling predictors\nconti_cols = ['age',\n 'campaign', 'duration']\n\nassembler_predictors = (\n    conti_cols + dummy_cols_job + dummy_cols_marital + dummy_cols_education +\n    dummy_cols_default + dummy_cols_housing + dummy_cols_loan\n)\n\n\ndtrain_pd = dtrain_pd[['y'] + assembler_predictors]\n\n\ndtrain_pd['y'].value_counts()\n\n\n\n\n\n\n\n\ncount\n\n\ny\n\n\n\n\n\n0\n24981\n\n\n1\n3156\n\n\n\n\ndtype: int64\n\n\n\n# Compute correlation matrix\ncorr_matrix = dtrain_pd.corr()\n\n# 3. Correlation heatmap using matplotlib\nfig, ax = plt.subplots(figsize=(12, 10))\ncax = ax.imshow(corr_matrix.values, aspect='auto')\nfig.colorbar(cax, ax=ax)\nax.set_xticks(range(len(corr_matrix.columns)))\nax.set_yticks(range(len(corr_matrix.columns)))\nax.set_xticklabels(corr_matrix.columns, rotation=90, fontsize=6)\nax.set_yticklabels(corr_matrix.columns, fontsize=6)\nplt.title('How Are Variables Correlated?')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndtrain_pd['y'].unique()\n\narray([0, 1])\n\n\n#Linear Regression\n\n# assembling predictors\nconti_cols = ['age',\n 'campaign', 'duration']\n\nassembler_predictors = (\n    conti_cols + dummy_cols_job + dummy_cols_marital + dummy_cols_education +\n    dummy_cols_default + dummy_cols_housing + dummy_cols_loan\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"y\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)\n\n# makting regression table\nprint( regression_table(model_1, assembler_1) )\n\n+-------------------------------+--------+------+------------+---------+--------------+--------------+\n| y: y                          |   Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-------------------------------+--------+------+------------+---------+--------------+--------------+\n| age                           |  0.001 | ***  |      0.001 |   0.000 |        0.000 |        0.003 |\n| campaign                      | -0.004 | ***  |      0.000 |   0.000 |       -0.004 |       -0.004 |\n| duration                      |  0.000 | ***  |      0.006 |   0.000 |       -0.012 |        0.013 |\n| job_blue-collar               | -0.037 | ***  |      0.010 |   0.000 |       -0.056 |       -0.018 |\n| job_entrepreneur              | -0.042 | ***  |      0.012 |   0.000 |       -0.065 |       -0.019 |\n| job_housemaid                 | -0.004 |      |      0.007 |   0.752 |       -0.018 |        0.011 |\n| job_management                | -0.020 | ***  |      0.010 |   0.006 |       -0.040 |        0.000 |\n| job_retired                   |  0.107 | ***  |      0.010 |   0.000 |        0.088 |        0.127 |\n| job_self-employed             | -0.025 |  **  |      0.007 |   0.010 |       -0.039 |       -0.012 |\n| job_services                  | -0.033 | ***  |      0.012 |   0.000 |       -0.057 |       -0.008 |\n| job_student                   |  0.142 | ***  |      0.006 |   0.000 |        0.130 |        0.153 |\n| job_technician                | -0.016 | ***  |      0.011 |   0.010 |       -0.038 |        0.007 |\n| job_unemployed                |  0.033 | ***  |      0.019 |   0.004 |       -0.005 |        0.070 |\n| job_unknown                   | -0.005 |      |      0.006 |   0.807 |       -0.016 |        0.006 |\n| marital_married               |  0.018 | ***  |      0.006 |   0.002 |        0.005 |        0.030 |\n| marital_single                |  0.044 | ***  |      0.037 |   0.000 |       -0.028 |        0.117 |\n| marital_unknown               |  0.044 |      |      0.009 |   0.238 |        0.026 |        0.061 |\n| education_basic_6y            | -0.000 |      |      0.007 |   0.971 |       -0.014 |        0.014 |\n| education_basic_9y            | -0.014 |  **  |      0.007 |   0.046 |       -0.029 |        0.000 |\n| education_high_school         | -0.006 |      |      0.076 |   0.450 |       -0.154 |        0.143 |\n| education_illiterate          |  0.096 |      |      0.008 |   0.206 |        0.079 |        0.112 |\n| education_professional_course |  0.008 |      |      0.008 |   0.345 |       -0.007 |        0.023 |\n| education_university_degree   |  0.022 | ***  |      0.010 |   0.004 |        0.002 |        0.042 |\n| education_unknown             |  0.017 |      |      0.004 |   0.107 |        0.008 |        0.025 |\n| default_unknown               | -0.060 | ***  |      0.283 |   0.000 |       -0.615 |        0.494 |\n| default_yes                   | -0.074 |      |      0.003 |   0.795 |       -0.080 |       -0.067 |\n| housing_yes                   |  0.006 |  *   |      0.005 |   0.077 |       -0.003 |        0.015 |\n| loan_yes                      | -0.004 |      |      0.014 |   0.345 |       -0.031 |        0.022 |\n| Intercept                     | -0.067 | ***  |      0.000 |         |       -0.067 |       -0.066 |\n------------------------------------------------------------------------------------------------------\n| Observations                  | 28,137 |      |            |         |              |              |\n| R²                            |  0.197 |      |            |         |              |              |\n| RMSE                          |  0.283 |      |            |         |              |              |\n+-------------------------------+--------+------+------------+---------+--------------+--------------+\n\n\n\nmodel_1.coefficients\n\nDenseVector([0.0013, -0.0035, 0.0005, -0.0374, -0.0419, -0.0037, -0.0201, 0.1075, -0.0253, -0.0327, 0.1416, -0.0156, 0.0326, -0.0047, 0.0176, 0.0444, 0.0437, -0.0003, -0.0144, -0.0056, 0.0958, 0.0079, 0.0218, 0.0165, -0.0604, -0.0736, 0.006, -0.0044])\n\n\n\nmodel_1.intercept\n\n-0.06683007113139161\n\n\n\nmodel_1.summary.coefficientStandardErrors\n\n[0.00020689461514424284,\n 0.0006171705306687189,\n 6.599702383709366e-06,\n 0.006268239893884113,\n 0.009681367787816423,\n 0.01173249429334274,\n 0.007295164229308897,\n 0.010315055581847624,\n 0.0098437016156203,\n 0.006786817771694096,\n 0.012461643911155625,\n 0.006031975051815466,\n 0.01138068449212235,\n 0.01904501315154409,\n 0.005552481450653328,\n 0.006387718004761517,\n 0.036970486611913794,\n 0.009030199308073547,\n 0.007210150938291389,\n 0.0074432359151217105,\n 0.07584875779542072,\n 0.008384793168323442,\n 0.007555834890730833,\n 0.010246290779180421,\n 0.004360092449459194,\n 0.2830236103087795,\n 0.003390331336245513,\n 0.004678154223777586,\n 0.013597780445066956]\n\n\n#Logistic Regression\n\n# assembling predictors\nx_cols = ['age',\n 'campaign', 'duration']\n\nassembler_predictors = (\n    conti_cols + dummy_cols_job + dummy_cols_marital + dummy_cols_education +\n    dummy_cols_default + dummy_cols_housing + dummy_cols_loan\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training the model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"y\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n\n# making prediction on both training and test\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)\n\n\nmodel_1.summary\n\nCoefficients:\n             Feature Estimate   Std Error  T Value P Value\n         (Intercept)  -3.8238      0.1770 -21.6010  0.0000\n                 age   0.0151      0.0026   5.8379  0.0000\n            campaign  -0.1296      0.0131  -9.9012  0.0000\n            duration   0.0040      0.0001  52.0245  0.0000\n     job_blue-collar  -0.5617      0.0856  -6.5638  0.0000\n    job_entrepreneur  -0.5592      0.1362  -4.1043  0.0000\n       job_housemaid  -0.0275      0.1514  -0.1816  0.8559\n      job_management  -0.2330      0.0898  -2.5928  0.0095\n         job_retired   0.8392      0.1080   7.7732  0.0000\n   job_self-employed  -0.2974      0.1275  -2.3328  0.0197\n        job_services  -0.4705      0.0944  -4.9859  0.0000\n         job_student   1.0812      0.1163   9.2963  0.0000\n      job_technician  -0.1934      0.0743  -2.6012  0.0093\n      job_unemployed   0.3868      0.1281   3.0187  0.0025\n         job_unknown  -0.0458      0.2466  -0.1859  0.8526\n     marital_married   0.2190      0.0752   2.9098  0.0036\n      marital_single   0.5574      0.0844   6.6062  0.0000\n     marital_unknown   0.4784      0.4271   1.1200  0.2627\n  education_basic_6y  -0.0709      0.1292  -0.5487  0.5832\n  education_basic_9y  -0.2534      0.1011  -2.5068  0.0122\neducation_high_sc...  -0.1090      0.0966  -1.1284  0.2591\neducation_illiterate   0.9843      0.7724   1.2743  0.2026\neducation_profess...   0.0683      0.1064   0.6419  0.5209\neducation_univers...   0.2100      0.0953   2.2038  0.0275\n   education_unknown   0.1611      0.1263   1.2754  0.2022\n     default_unknown  -1.0002      0.0708 -14.1205  0.0000\n         default_yes -24.0285 356123.9993  -0.0001  0.9999\n         housing_yes   0.0653      0.0433   1.5102  0.1310\n            loan_yes  -0.0793      0.0610  -1.3008  0.1933\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 19753.2417 on 28108 degrees of freedom\nResidual deviance: 15256.4738 on 28108 degrees of freedom\nAIC: 15314.4738\n\n\n#Regularized Logistic Regression\n\n# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndf_pd\n\nWarning: total number of rows (40198) exceeds max_rows (20000). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\nage\njob\nmarital\neducation\ndefault\nhousing\nloan\ncontact\nmonth\nday_of_week\n...\ncampaign\npdays\nprevious\npoutcome\nemp.var.rate\ncons.price.idx\ncons.conf.idx\neuribor3m\nnr.employed\ny\n\n\n\n\n0\n56\nhousemaid\nmarried\nbasic_4y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\n0\n\n\n1\n57\nservices\nmarried\nhigh_school\nunknown\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\n0\n\n\n2\n37\nservices\nmarried\nhigh_school\nno\nyes\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\n0\n\n\n3\n40\nadmin_\nmarried\nbasic_6y\nno\nno\nno\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\n0\n\n\n4\n56\nservices\nmarried\nhigh_school\nno\nno\nyes\ntelephone\nmay\nmon\n...\n1\n999\n0\nnonexistent\n1.1\n93.994\n-36.4\n4.857\n5191.0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41183\n73\nretired\nmarried\nprofessional_course\nno\nyes\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\n1\n\n\n41184\n46\nblue-collar\nmarried\nprofessional_course\nno\nno\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\n0\n\n\n41185\n56\nretired\nmarried\nuniversity_degree\nno\nyes\nno\ncellular\nnov\nfri\n...\n2\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\n0\n\n\n41186\n44\ntechnician\nmarried\nprofessional_course\nno\nno\nno\ncellular\nnov\nfri\n...\n1\n999\n0\nnonexistent\n-1.1\n94.767\n-50.8\n1.028\n4963.6\n1\n\n\n41187\n74\nretired\nmarried\nprofessional_course\nno\nyes\nno\ncellular\nnov\nfri\n...\n3\n999\n1\nfailure\n-1.1\n94.767\n-50.8\n1.028\n4963.6\n0\n\n\n\n\n40198 rows × 21 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndtrain, dtest = df.randomSplit([0.7, 0.3], seed = 1234)\n\n\ndf_pd['job'].unique()\n\narray(['housemaid', 'services', 'admin_', 'blue-collar', 'technician',\n       'retired', 'management', 'unemployed', 'self-employed', 'unknown',\n       'entrepreneur', 'student'], dtype=object)\n\n\n\ndf_pd['marital'].unique()\n\narray(['married', 'single', 'divorced', 'unknown'], dtype=object)\n\n\n\ndf_pd['education'].unique()\n\narray(['basic_4y', 'high_school', 'basic_6y', 'basic_9y',\n       'professional_course', 'unknown', 'university_degree',\n       'illiterate'], dtype=object)\n\n\n\ndf_pd['default'].unique()\n\narray(['no', 'unknown', 'yes'], dtype=object)\n\n\n\ndf_pd['housing'].unique()\n\narray(['no', 'yes'], dtype=object)\n\n\n\ndf_pd['loan'].unique()\n\narray(['no', 'yes'], dtype=object)\n\n\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\ncustom_order_job = ['admin_','housemaid', 'services', 'blue-collar', 'technician',\n       'retired', 'management', 'unemployed', 'self-employed', 'unknown',\n       'entrepreneur', 'student']\ncustom_order_marital = ['divorced', 'married', 'single', 'unknown']\ncustom_order_education = ['basic_4y', 'high_school', 'basic_6y', 'basic_9y',\n       'professional_course', 'unknown', 'university_degree',\n       'illiterate']\ncustom_order_default = ['no', 'unknown', 'yes']\ncustom_order_housing = ['no', 'yes']\ncustom_order_loan = ['no', 'yes']\ndummy_cols_job, ref_category_job = add_dummy_variables('job', 0, category_order = custom_order_job)\ndummy_cols_marital, ref_category_marital = add_dummy_variables('marital', 0, category_order = custom_order_marital)\ndummy_cols_education, ref_category_education = add_dummy_variables('education', 0, category_order = custom_order_education)\ndummy_cols_default, ref_category_default = add_dummy_variables('default', 0, category_order = custom_order_default)\ndummy_cols_housing, ref_category_housing = add_dummy_variables('housing', 0, category_order = custom_order_housing)\ndummy_cols_loan, ref_category_loan = add_dummy_variables('loan', 0, category_order = custom_order_loan)\n\nReference category (dummy omitted): admin_\nReference category (dummy omitted): divorced\nReference category (dummy omitted): basic_4y\nReference category (dummy omitted): no\nReference category (dummy omitted): no\nReference category (dummy omitted): no\n\n\n\n# Keep the name assembler_predictors unchanged,\n#   as it will be used as a global variable in the marginal_effects UDF.\nconti_cols = ['age',\n 'campaign', 'duration']\n\nassembler_predictors = (\n    conti_cols + dummy_cols_job + dummy_cols_marital + dummy_cols_education +\n    dummy_cols_default + dummy_cols_housing + dummy_cols_loan\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n\n# training the model\nmodel_1 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"y\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_1)\n)\n\n\n# making prediction on both training and test\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)\n\n#dtrain_1 = dtrain_1.withColumnRenamed(\"prediction\", \"prediction_lr\")\n#dtest_1 = dtest_1.withColumnRenamed(\"prediction\", \"prediction_lr\")\n\n\nmodel_1.summary\n\nCoefficients:\n             Feature Estimate   Std Error  T Value P Value\n         (Intercept)  -3.8238      0.1770 -21.6010  0.0000\n                 age   0.0151      0.0026   5.8379  0.0000\n            campaign  -0.1296      0.0131  -9.9012  0.0000\n            duration   0.0040      0.0001  52.0245  0.0000\n       job_housemaid  -0.0275      0.1514  -0.1816  0.8559\n        job_services  -0.4705      0.0944  -4.9859  0.0000\n     job_blue-collar  -0.5617      0.0856  -6.5638  0.0000\n      job_technician  -0.1934      0.0743  -2.6012  0.0093\n         job_retired   0.8392      0.1080   7.7732  0.0000\n      job_management  -0.2330      0.0898  -2.5928  0.0095\n      job_unemployed   0.3868      0.1281   3.0187  0.0025\n   job_self-employed  -0.2974      0.1275  -2.3328  0.0197\n         job_unknown  -0.0458      0.2466  -0.1859  0.8526\n    job_entrepreneur  -0.5592      0.1362  -4.1043  0.0000\n         job_student   1.0812      0.1163   9.2963  0.0000\n     marital_married   0.2190      0.0752   2.9098  0.0036\n      marital_single   0.5574      0.0844   6.6062  0.0000\n     marital_unknown   0.4784      0.4271   1.1200  0.2627\neducation_high_sc...  -0.1090      0.0966  -1.1284  0.2591\n  education_basic_6y  -0.0709      0.1292  -0.5487  0.5832\n  education_basic_9y  -0.2534      0.1011  -2.5068  0.0122\neducation_profess...   0.0683      0.1064   0.6419  0.5209\n   education_unknown   0.1611      0.1263   1.2754  0.2022\neducation_univers...   0.2100      0.0953   2.2038  0.0275\neducation_illiterate   0.9843      0.7724   1.2743  0.2026\n     default_unknown  -1.0002      0.0708 -14.1205  0.0000\n         default_yes -24.0285 356123.9993  -0.0001  0.9999\n         housing_yes   0.0653      0.0433   1.5102  0.1310\n            loan_yes  -0.0793      0.0610  -1.3008  0.1933\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 19753.2417 on 28108 degrees of freedom\nResidual deviance: 15256.4738 on 28108 degrees of freedom\nAIC: 15314.4738\n\n\n\n# Filter training data for atRisk == 1 and atRisk == 0\npdf = dtrain_1.select(\"prediction\", \"y\").toPandas()\n\ntrain_true = pdf[pdf[\"y\"] == 1]\ntrain_false = pdf[pdf[\"y\"] == 0]\n\n# Create the first density plot\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions\")\nplt.legend(title=\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Define threshold for vertical line\nthreshold = 0.11  # Replace with actual value\n\n# Create the second density plot with vertical line\nplt.figure(figsize=(8, 6))\nsns.kdeplot(train_true[\"prediction\"], label=\"TRUE\", color=\"red\", fill=True)\nsns.kdeplot(train_false[\"prediction\"], label=\"FALSE\", color=\"blue\", fill=True)\nplt.axvline(x=threshold, color=\"blue\", linestyle=\"dashed\", label=f\"Threshold = {threshold}\")\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Density\")\nplt.title(\"Density Plot of Predictions with Threshold\")\nplt.legend(title=\"y\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Compute confusion matrix\ndtest_1 = dtest_1.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; threshold, 1).otherwise(0))\nconf_matrix = dtest_1.groupBy(\"y\", \"predicted_class\").count().orderBy(\"y\", \"predicted_class\")\n\nTP = dtest_1.filter((col(\"y\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_1.filter((col(\"y\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_1.filter((col(\"y\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_1.filter((col(\"y\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |     8496   |     2188  |\n------------+------------+------------\nActual Pos. |      323   |     1054  |\n------------+------------+------------\nAccuracy:  0.7918\nPrecision: 0.3251\nRecall (Sensitivity): 0.7654\nSpecificity:  0.7952\nAverage Rate: 0.1142\nEnrichment:   2.8476 (Relative Precision)\n\n\n\npdf = dtest_1.select(\"prediction\", \"y\").toPandas()\n\n# Extract predictions and true labels\ny_true = pdf[\"y\"]  # True labels\ny_scores = pdf[\"prediction\"]  # Predicted probabilities\n\n# Compute precision, recall, and thresholds\nprecision_plot, recall_plot, thresholds = precision_recall_curve(y_true, y_scores)\n\n# Compute enrichment: precision divided by average at-risk rate\naverage_rate = np.mean(y_true)\nenrichment_plot = precision_plot / average_rate\n\n# Define optimal threshold (example: threshold where recall ≈ enrichment balance)\nthreshold = 0.79  # Adjust based on the plot\n\n# Plot Enrichment vs. Recall vs. Threshold\nplt.figure(figsize=(8, 6))\nplt.plot(thresholds, enrichment_plot[:-1], label=\"Enrichment\", color=\"blue\", linestyle=\"--\")\nplt.plot(thresholds, recall_plot[:-1], label=\"Recall\", color=\"red\", linestyle=\"-\")\n\n# Add vertical line for chosen threshold\nplt.axvline(x=threshold, color=\"black\", linestyle=\"dashed\", label=f\"Optimal Threshold = {threshold}\")\n\n# Labels and legend\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Score\")\nplt.title(\"Enrichment vs. Recall\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"y\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_1)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_1.select(\"prediction\", \"y\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"y\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.8609\n\n\n\n\n\n\n\n\n\n#Lasso Logistic Regression\n\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\n\ndtrain_pd = dtrain.toPandas()\ndtest_pd = dtest.toPandas()\n\n\nassembler_predictors\n\n['age',\n 'campaign',\n 'duration',\n 'job_housemaid',\n 'job_services',\n 'job_blue-collar',\n 'job_technician',\n 'job_retired',\n 'job_management',\n 'job_unemployed',\n 'job_self-employed',\n 'job_unknown',\n 'job_entrepreneur',\n 'job_student',\n 'marital_married',\n 'marital_single',\n 'marital_unknown',\n 'education_high_school',\n 'education_basic_6y',\n 'education_basic_9y',\n 'education_professional_course',\n 'education_unknown',\n 'education_university_degree',\n 'education_illiterate',\n 'default_unknown',\n 'default_yes',\n 'housing_yes',\n 'loan_yes']\n\n\n\ndtrain_pd = dtrain_pd[assembler_predictors+['y']]\ndtest_pd = dtest_pd[assembler_predictors+['y']]\n\n\ndtrain_pd\n\nWarning: total number of rows (28137) exceeds max_rows (20000). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\nage\ncampaign\nduration\njob_housemaid\njob_services\njob_blue-collar\njob_technician\njob_retired\njob_management\njob_unemployed\n...\neducation_basic_9y\neducation_professional_course\neducation_unknown\neducation_university_degree\neducation_illiterate\ndefault_unknown\ndefault_yes\nhousing_yes\nloan_yes\ny\n\n\n\n\n0\n20\n1\n285\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n20\n2\n217\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n20\n3\n598\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n20\n1\n680\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n20\n15\n97\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n28132\n92\n1\n1064\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n\n\n28133\n92\n1\n370\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n\n\n28134\n94\n1\n134\n0\n0\n0\n0\n1\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n28135\n95\n1\n85\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n28136\n98\n1\n476\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n1\n\n\n\n\n28137 rows × 29 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ny_train = dtrain_pd['y']\ny_test = dtest_pd['y']\n\n\nX_train = dtrain_pd[assembler_predictors]\nX_test = dtest_pd[assembler_predictors]\n\n\n# Revised LogisticRegressionCV with fewer candidate Cs, fewer folds, and looser tolerance:\nlasso_cv = LogisticRegressionCV(\n    Cs=10,         # Fewer candidate values\n    cv=3,          # Fewer CV folds\n    penalty='l1',\n    solver='saga',\n    max_iter=1000,\n    tol=1e-3,      # Looser tolerance for faster convergence\n    scoring='neg_log_loss'\n)\nlasso_cv.fit(X_train.values, y_train.values)\n\nprint(\"Best alpha:\", 1 / lasso_cv.C_[0])\n\nintercept = float(lasso_cv.intercept_[0])\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(lasso_cv.coef_[0]),\n    'exp_coefficient': np.exp( list(lasso_cv.coef_[0]) ),\n})\n\nBest alpha: 0.0001\n\n\n\nnp.exp(lasso_cv.intercept_[0])\n\nnp.float64(0.9389862225540642)\n\n\n\ncoef_lasso = coef_lasso.query('coefficient != 0')\n\n\nintercept\n\n-0.06295447234790122\n\n\n\n# Note: solver='saga' supports L1 regularization.\nlasso_cv = LogisticRegressionCV(\n    Cs=100, cv=5, penalty='l1', solver='saga', max_iter=1000, scoring='neg_log_loss'\n)\nlasso_cv.fit(X_train, y_train)\n\nintercept = float(lasso_cv.intercept_)\ncoef_lasso = pd.DataFrame({\n    'predictor': list(X_train.columns),\n    'coefficient': list(lasso_cv.coef_[0])\n})\n\nprint(\"Lasso Regression Coefficients:\")\nprint(coef_lasso)\n\n# Force an order for the y-axis (using the feature names as they appear in coef_lasso)\norder = coef_lasso['predictor'].tolist()\n\nplt.figure(figsize=(8,6))\nax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_lasso, order=order, join=False)\nplt.title(\"Coefficients of Lasso Logistic Regression Model\")\nplt.xlabel(\"Coefficient value\")\nplt.ylabel(\"Predictor\")\n\n# Draw horizontal lines from 0 to each coefficient.\nfor _, row in coef_lasso.iterrows():\n    # Get the y-axis position from the order list.\n    y_pos = order.index(row['predictor'])\n    plt.hlines(y=y_pos, xmin=0, xmax=row['coefficient'], color='gray', linestyle='--')\n\n# Draw a vertical line at 0.\nplt.axvline(0, color='black', linestyle='--')\n\nplt.show()\n\n# Prediction and evaluation for lasso model\ny_pred_prob_lasso = lasso_cv.predict_proba(X_test)[:, 1]\ny_pred_lasso = (y_pred_prob_lasso &gt; 0.5).astype(int)\nctab_lasso = confusion_matrix(y_test, y_pred_lasso)\naccuracy_lasso = accuracy_score(y_test, y_pred_lasso)\nprecision_lasso = precision_score(y_test, y_pred_lasso)\nrecall_lasso = recall_score(y_test, y_pred_lasso)\nauc_lasso = roc_auc_score(y_test, y_pred_prob_lasso)\n\nprint(\"Confusion Matrix (Lasso):\\n\", ctab_lasso)\nprint(\"Lasso Accuracy:\", accuracy_lasso)\nprint(\"Lasso Precision:\", precision_lasso)\nprint(\"Lasso Recall:\", recall_lasso)\n\n\n# Plot ROC Curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_lasso)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f'Lasso (AUC = {auc_lasso:.2f})')\nplt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Lasso Logistic Regression Model')\nplt.legend(loc='best')\nplt.show()\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n&lt;ipython-input-76-41c5756d4c3e&gt;:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  intercept = float(lasso_cv.intercept_)\n&lt;ipython-input-76-41c5756d4c3e&gt;:20: UserWarning: \n\nThe `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n\n  ax = sns.pointplot(x=\"coefficient\", y=\"predictor\", data=coef_lasso, order=order, join=False)\n\n\nLasso Regression Coefficients:\n                        predictor  coefficient\n0                             age    -0.033978\n1                        campaign    -0.235021\n2                        duration     0.003321\n3                   job_housemaid     0.000206\n4                    job_services    -0.093253\n5                 job_blue-collar    -0.281852\n6                  job_technician    -0.047449\n7                     job_retired     0.182412\n8                  job_management    -0.003973\n9                  job_unemployed     0.003045\n10              job_self-employed    -0.005532\n11                    job_unknown    -0.000251\n12               job_entrepreneur    -0.011212\n13                    job_student     0.019067\n14                marital_married    -0.143780\n15                 marital_single    -0.090573\n16                marital_unknown     0.000143\n17          education_high_school    -0.115260\n18             education_basic_6y    -0.016910\n19             education_basic_9y    -0.130972\n20  education_professional_course    -0.012714\n21              education_unknown     0.005362\n22    education_university_degree     0.025389\n23           education_illiterate     0.000179\n24                default_unknown    -0.235477\n25                    default_yes    -0.000010\n26                    housing_yes    -0.065277\n27                       loan_yes    -0.020673\n\n\n\n\n\n\n\n\n\nConfusion Matrix (Lasso):\n [[10525   159]\n [ 1131   246]]\nLasso Accuracy: 0.8930436945526905\nLasso Precision: 0.6074074074074074\nLasso Recall: 0.1786492374727669\n\n\n\n\n\n\n\n\n\n#Prunned Tree\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.inspection import PartialDependenceDisplay\n\n\n# In scikit-learn, we can use min_impurity_decrease=0.005 for a similar effect.\ntree_model = DecisionTreeRegressor(min_impurity_decrease=0.0005, random_state=42)\n# Fit the model using all predictors (all columns except 'medv')\ntree_model.fit(X_train, y_train)\n\n\n# Predict on training and test sets\ny_train_pred = tree_model.predict(X_train)\ny_test_pred = tree_model.predict(X_test)\n\n# Calculate MSE\nmse_train = mean_squared_error(y_train, y_train_pred)\nmse_test = mean_squared_error(y_test, y_test_pred)\n\n# Print the results\nprint(f\"Training MSE: {mse_train:.3f}\")\nprint(f\"Test MSE: {mse_test:.3f}\")\n\n# Plot the initial regression tree\nplt.figure(figsize=(12, 8))\nplot_tree(tree_model, feature_names=X_train.columns, filled=True, rounded=True)\nplt.title(\"Regression Tree for y (Initial Fit)\")\nplt.show()\n\nTraining MSE: 0.078\nTest MSE: 0.079\n\n\n\n\n\n\n\n\n\n#Random Forest\n\n\n# Build the Random Forest model\n# max_features=13 means that at each split the algorithm randomly considers 13 predictors.\nrf = RandomForestRegressor(max_features=5,  # Use 13 features at each split\n                           n_estimators=500,  # Number of trees in the forest\n                           random_state=42,\n                           oob_score=True)    # Use out-of-bag samples to estimate error\nrf.fit(X_train, y_train)\n\n\n# Print the model details\nprint(\"Random Forest Model:\")\nprint(rf)\n\n# Output the model details (feature importances, OOB score, etc.)\nprint(\"Out-of-bag score:\", rf.oob_score_)  # A rough estimate of generalization error\n\n\n# Generate predictions on training and testing sets\ny_train_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)\n\n# Calculate Mean Squared Errors (MSE) for both sets\ntrain_mse = mean_squared_error(y_train, y_train_pred)\ntest_mse = mean_squared_error(y_test, y_test_pred)\nprint(\"Train MSE:\", train_mse)\nprint(\"Test MSE:\", test_mse)\n\n# Optional: Plot predicted vs. observed values for test data\nplt.figure(figsize=(8,6), dpi=300)\nplt.scatter(y_test, y_test_pred, alpha=0.7)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\nplt.xlabel(\"Observed y\")\nplt.ylabel(\"Predicted y\")\nplt.title(\"Random Forest: Observed vs. Predicted Values\")\nplt.show()\n\nRandom Forest Model:\nRandomForestRegressor(max_features=5, n_estimators=500, oob_score=True,\n                      random_state=42)\nOut-of-bag score: 0.17876557294409545\nTrain MSE: 0.011213214834019861\nTest MSE: 0.08252805794007545\n\n\n\n\n\n\n\n\n\n\n# Get feature importances from the model (equivalent to importance(bag.boston) in R)\nimportances = rf.feature_importances_\nfeature_names = X_train.columns\n\nprint(\"Variable Importances:\")\nfor name, imp in zip(feature_names, importances):\n    print(f\"{name}: {imp:.4f}\")\n\n# Plot the feature importances, similar to varImpPlot(bag.boston) in R\n# Sort the features by importance for a nicer plot.\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(10, 6), dpi=150)\nplt.title(\"Variable Importances\")\nplt.bar(range(len(feature_names)), importances[indices], align='center')\nplt.xticks(range(len(feature_names)), feature_names[indices], rotation=90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.show()\n\nVariable Importances:\nage: 0.2286\ncampaign: 0.0716\nduration: 0.4865\njob_services: 0.0059\njob_admin_: 0.0100\njob_blue-collar: 0.0075\njob_technician: 0.0091\njob_retired: 0.0071\njob_management: 0.0067\njob_unemployed: 0.0055\njob_self-employed: 0.0048\njob_unknown: 0.0020\njob_entrepreneur: 0.0045\njob_student: 0.0069\nmarital_single: 0.0143\nmarital_divorced: 0.0126\nmarital_unknown: 0.0014\neducation_high_school: 0.0113\neducation_basic_6y: 0.0058\neducation_basic_9y: 0.0085\neducation_professional_course: 0.0096\neducation_unknown: 0.0063\neducation_university_degree: 0.0107\neducation_illiterate: 0.0004\ndefault_unknown: 0.0137\ndefault_yes: 0.0000\nhousing_yes: 0.0289\nloan_yes: 0.0200\n\n\n\n\n\n\n\n\n\n\nPartialDependenceDisplay.from_estimator(rf, X_train, ['duration'], kind='both')\n\n\n\n\n\n\n\n\n\nPartialDependenceDisplay.from_estimator(rf, X_train, ['age'], kind='both')\n\n\n\n\n\n\n\n\n\ndisp = PartialDependenceDisplay.from_estimator(rf, X_train, ['duration'], kind='both')\n\n# Access the line representing the average PDP (it's typically the last Line2D object)\n# and change its color manually\nfor ax in disp.axes_.ravel():\n    lines = ax.get_lines()\n    if lines:  # In case the axis has line objects\n        # The last line is usually the average PDP\n        pdp_line = lines[-1]\n        pdp_line.set_color(\"red\")  # Change to any color you like\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndisp = PartialDependenceDisplay.from_estimator(rf, X_train, ['age'], kind='both')\n\n# Access the line representing the average PDP (it's typically the last Line2D object)\n# and change its color manually\nfor ax in disp.axes_.ravel():\n    lines = ax.get_lines()\n    if lines:  # In case the axis has line objects\n        # The last line is usually the average PDP\n        pdp_line = lines[-1]\n        pdp_line.set_color(\"red\")  # Change to any color you like\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the grid of hyperparameters:\n# - min_samples_leaf is the minimum number of samples in a terminal node.\nparam_grid = {\n    \"max_features\": list(range(3, 15, 2)),\n    \"min_samples_leaf\": [5]\n}\n\n# Initialize the RandomForestRegressor:\n# - n_estimators is set to 50 (equivalent to num.trees)\n# - random_state is set for reproducibility.\n# rf = RandomForestRegressor(n_estimators=50, random_state=1917)\n\nrf = RandomForestRegressor(n_estimators=500,  # Number of trees in the forest\n                           random_state=42,\n                           oob_score=True)    # Use out-of-bag samples to estimate error\n\n# Set up 10-fold cross-validation and GridSearch over the parameters\ngrid_search = GridSearchCV(\n    estimator=rf,\n    param_grid=param_grid,\n    cv=10,\n    scoring=\"neg_mean_squared_error\",\n    return_train_score=True,\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit the grid search on the data\ngrid_search.fit(X_train, y_train)\n\n# Extract the best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# To replicate the ggplot visualization from R, we plot the grid search results.\nresults = pd.DataFrame(grid_search.cv_results_)\n\nplt.figure(figsize=(8, 6))\nplt.errorbar(\n    results[\"param_max_features\"].astype(int),\n    -results[\"mean_test_score\"],\n    yerr=results[\"std_test_score\"],\n    fmt=\"o-\",\n    capsize=5\n)\nplt.title(\"Grid Search CV Results\")\nplt.xlabel(\"max_features (mtry equivalent)\")\nplt.ylabel(\"Mean Squared Error\")\nplt.grid(True)\nplt.show()\n\nFitting 10 folds for each of 6 candidates, totalling 60 fits\nBest Parameters: {'max_features': 3, 'min_samples_leaf': 5}\n\n\n\n\n\n\n\n\n\n#Gradient Boosting\n\n# Define the grid of hyperparameters\nparam_grid = {\n    \"n_estimators\": list(range(20, 201, 20)),  # nrounds: 20, 40, ..., 200\n    \"learning_rate\": [0.025, 0.05, 0.1, 0.3],  # eta\n    \"gamma\": [0],                              # gamma\n    \"max_depth\": [1, 2, 3, 4],\n    \"colsample_bytree\": [1],\n    \"min_child_weight\": [1],\n    \"subsample\": [1]\n}\n\n# Initialize the XGBRegressor with the regression objective and fixed random state for reproducibility\nxgb_reg = XGBRegressor(objective=\"reg:squarederror\", random_state=1937, verbosity=1)\n\n# Set up GridSearchCV with 10-fold cross-validation; scoring is negative MSE\ngrid_search = GridSearchCV(\n    estimator=xgb_reg,\n    param_grid=param_grid,\n    scoring=\"neg_mean_squared_error\",\n    cv=10,\n    verbose=1  # Adjust verbosity as needed\n)\n\n# Fit the grid search\ngrid_search.fit(X_train, y_train)\n\n# Train the final model using the best parameters (grid_search.best_estimator_ is already refit on entire data)\nfinal_model = grid_search.best_estimator_\n\n# Plot variable importance using XGBoost's plot_importance function\nplt.figure(figsize=(10, 8))\nplot_importance(final_model)\nplt.title(\"Variable Importance\")\nplt.show()\n\n# Calculate MSE on the test data\ny_pred = final_model.predict(X_test)\ntest_mse = mean_squared_error(y_test, y_pred)\nprint(\"Test MSE:\", test_mse)\n\n# Print the best parameters found by GridSearchCV\nbest_params = grid_search.best_params_\nprint(\"Best parameters:\", best_params)\n\nFitting 10 folds for each of 160 candidates, totalling 1600 fits\n\n\n&lt;Figure size 1000x800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nTest MSE: 0.08096029609441757\nBest parameters: {'colsample_bytree': 1, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 1, 'min_child_weight': 1, 'n_estimators': 40, 'subsample': 1}"
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "Housing_Markets_danl_320_hw4_Xu_Daniel.html",
    "href": "Housing_Markets_danl_320_hw4_Xu_Daniel.html",
    "title": "Housing Markets",
    "section": "",
    "text": "# Below is for an interactive display of Pandas DataFrame in Colab\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .4f}\",\n            significance_stars(p_value),\n            f\"{std_error: .4f}\",\n            f\"{ci_lower: .4f}\",\n            f\"{ci_upper: .4f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "Housing_Markets_danl_320_hw4_Xu_Daniel.html#udfs",
    "href": "Housing_Markets_danl_320_hw4_Xu_Daniel.html#udfs",
    "title": "Housing Markets",
    "section": "",
    "text": "def regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\n\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .4f}\",\n            significance_stars(p_value),\n            f\"{std_error: .4f}\",\n            f\"{ci_lower: .4f}\",\n            f\"{ci_upper: .4f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)"
  },
  {
    "objectID": "danl-320-python-basic.html",
    "href": "danl-320-python-basic.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-320-python-basic.html#what-is-python",
    "href": "danl-320-python-basic.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "danl-320-python-basic.html#variables-and-data-types",
    "href": "danl-320-python-basic.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-320-python-basic.html#control-structures",
    "href": "danl-320-python-basic.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "danl-320-python-basic.html#functions",
    "href": "danl-320-python-basic.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "danl-320-python-basic.html#lists-and-dictionaries",
    "href": "danl-320-python-basic.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Insightful Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nUDF and Imports\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSettings\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Bank Term Deposit Success with Machine Learning\n\n\n\n\n\n\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\nHousing Markets\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nDANL 320 Project\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nDaniel\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarwars\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYour Name\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl_proj_nba.html#salary-distribution-among-teams",
    "href": "danl_proj_nba.html#salary-distribution-among-teams",
    "title": "Data Analysis Project",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet’s start with the salary distribution among teams using seaborn for visualization. ​​\n\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n/var/folders/_m/d6jf0jhd2zzdfd5kzdhl_24w0000gn/T/ipykernel_79892/1671011424.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  nba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 16))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams.\nNotice that Portland Trail Blazers has the highest total salary followed by Golden State Warriors and Philadelphia 76ers, and Memphis Grizzlies has the lowest total salary."
  },
  {
    "objectID": "danl_proj_nba.html#player-age-distribution",
    "href": "danl_proj_nba.html#player-age-distribution",
    "title": "Data Analysis Project",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let’s explore the Player Age Distribution across the NBA. We’ll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ​​\n\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\n# nba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\n# nba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n/Users/bchoe/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players.\nNotice that the majority of players fall within an age range from 24 to 34. There are few players whose age is above 40."
  },
  {
    "objectID": "danl_proj_nba.html#position-wise-salary-insights",
    "href": "danl_proj_nba.html#position-wise-salary-insights",
    "title": "Data Analysis Project",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we’ll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let’s create a box plot to visualize the salary distribution for each position. ​​\n\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. PG-SG has the highest median salary."
  },
  {
    "objectID": "danl_proj_nba.html#top-10-highest-paid-players",
    "href": "danl_proj_nba.html#top-10-highest-paid-players",
    "title": "Data Analysis Project",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we’ll identify the Top 10 Highest Paid Players in the NBA. Let’s visualize this information.\n\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'PlayerName',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Xu",
    "section": "",
    "text": "I am a full-time student at SUNY Geneseo, majoring in Economics with a minor in Data Analytics within the AACSB-accredited School of Business. I am actively seeking a Data Analyst or Business Analyst internship to gain hands-on experience and explore my passion for these fields.\nMy peers and mentors often describe me as hardworking, organized, and possessing strong communication skills. I am eager to contribute these qualities to a dynamic team."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Daniel Xu",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Economics | Aug 2021 - May 2025  Minor in Data Analytics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Daniel Xu",
    "section": "Experience",
    "text": "Experience\nResident Assistant (RA) | ResLife at Geneseo | Aug 2023 - Current"
  },
  {
    "objectID": "posts/Beer Markets/Beer Markets.html",
    "href": "posts/Beer Markets/Beer Markets.html",
    "title": "UDF and Imports",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-8-e88d64471ed3&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 terms = assembler3.getInputCols()\n      2 coefs = model3.coefficients.toArray()[:len(terms)]\n      3 stdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n      4 \n      5 df_summary = pd.DataFrame({\n\nNameError: name 'assembler3' is not defined\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)\n\n\nQ1\n\n\n\nimage.png\n\n\n\nbeer_markets = pd.read_csv('https://bcdanl.github.io/data/beer_markets_all_cleaned.csv')\n\nbeer_markets = beer_markets[beer_markets['container'].isin(['CAN', 'NON_REFILLABLE_BOTTLE'])]\ndf = spark.createDataFrame(beer_markets)\n\ndf.show()\n\n+---------+--------------------+--------+---------+------------+---------+------------------+--------------------+-----+-------+----------+----------------+---------+--------+--------------+-------------+---+----------+-------+--------------------+------+---------+----------+-------+----------------+-------+\n|household|     X_purchase_desc|quantity|    brand|dollar_spent|beer_floz|        price_floz|           container|promo| region|     state|          market|buyertype|  income|childrenUnder6|children6to17|age|employment| degree|          occupation|ethnic|microwave|dishwasher|tvcable|singlefamilyhome|npeople|\n+---------+--------------------+--------+---------+------------+---------+------------------+--------------------+-----+-------+----------+----------------+---------+--------+--------------+-------------+---+----------+-------+--------------------+------+---------+----------+-------+----------------+-------+\n|  2000946|    BUD LT BR CN 12P|       1|BUD_LIGHT|        8.14|    144.0|0.0565277777777777|                 CAN|false|CENTRAL|  ILLINOIS|  RURAL_ILLINOIS|     male|  20-60k|         false|        false|50+|      none|   Grad|none/retired/student| white|     true|      true|premium|           false|      1|\n|  2003036|    BUD LT BR CN 24P|       1|BUD_LIGHT|       17.48|    288.0|0.0606944444444444|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2003036|    BUD LT BR CN 24P|       2|BUD_LIGHT|       33.92|    576.0|0.0588888888888888|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2003036|    BUD LT BR CN 30P|       2|BUD_LIGHT|       34.74|    720.0|           0.04825|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2003036|    BUD LT BR CN 36P|       2|BUD_LIGHT|       40.48|    864.0|0.0468518518518518|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2003036|    BUD LT BR CN 36P|       2|BUD_LIGHT|       42.96|    864.0|0.0497222222222222|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2003036|    BUD LT BR CN 36P|       2|BUD_LIGHT|       40.96|    864.0|0.0474074074074074|                 CAN|false|  SOUTH|   GEORGIA|         ATLANTA|  married|100-200k|         false|        false|50+|      full|College|clerical/sales/se...| white|     true|      true|  basic|            true|      2|\n|  2001521|     BUD LT BR CN 6P|       5|BUD_LIGHT|        30.6|    480.0|           0.06375|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001521|     BUD LT BR CN 6P|       1|BUD_LIGHT|        9.99|     96.0|         0.1040625|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001521|     BUD LT BR CN 6P|       5|BUD_LIGHT|        30.7|    480.0|0.0639583333333333|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001521|     BUD LT BR CN 6P|       3|BUD_LIGHT|       18.36|    288.0|           0.06375|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001521|     BUD LT BR CN 6P|       5|BUD_LIGHT|       31.05|    480.0|         0.0646875|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001521|     BUD LT BR CN 6P|       1|BUD_LIGHT|        9.99|     96.0|         0.1040625|                 CAN|false|CENTRAL|   INDIANA|   RURAL_INDIANA|     male| 60-100k|         false|        false|50+|      none|College|none/retired/student| white|     true|      true|   none|           false|      1|\n|  2001978|        BUD LT BR CN|       2|BUD_LIGHT|        2.98|     48.0|0.0620833333333333|                 CAN|false|   EAST|  NEW YORK|        SYRACUSE|  married| 60-100k|         false|        false|50+|      part|     HS|clerical/sales/se...| white|     true|      true|   none|            true|      2|\n|  2001978|        BUD LT BR CN|       1|BUD_LIGHT|        1.49|     24.0|0.0620833333333333|                 CAN|false|   EAST|  NEW YORK|        SYRACUSE|  married| 60-100k|         false|        false|50+|      part|     HS|clerical/sales/se...| white|     true|      true|   none|            true|      2|\n|  2007716| BUD LT BR NRB LN 6P|       1|BUD_LIGHT|         3.0|     42.0|0.0714285714285714|NON_REFILLABLE_BO...|false|   EAST|NEW JERSEY|    SURBURBAN_NJ|  married|  20-60k|         false|        false|50+|      none|     HS|none/retired/student| white|     true|      true|premium|           false|      2|\n|  2007716| BUD LT BR NRB LN 6P|       1|BUD_LIGHT|        4.99|     72.0|0.0693055555555555|NON_REFILLABLE_BO...|false|   EAST|NEW JERSEY|    SURBURBAN_NJ|  married|  20-60k|         false|        false|50+|      none|     HS|none/retired/student| white|     true|      true|premium|           false|      2|\n|  2008311|BUD LT BR NRB LN 12P|       1|BUD_LIGHT|        9.99|    144.0|          0.069375|NON_REFILLABLE_BO...| true|  SOUTH|   GEORGIA|         ATLANTA|  married| 60-100k|         false|        false|50+|      full|     HS|                prof| white|     true|      true|   none|            true|      2|\n|  2008311|BUD LT BR NRB LN 12P|       1|BUD_LIGHT|        9.49|    144.0|0.0659027777777777|NON_REFILLABLE_BO...| true|  SOUTH|   GEORGIA|         ATLANTA|  married| 60-100k|         false|        false|50+|      full|     HS|                prof| white|     true|      true|   none|            true|      2|\n|  2010861|    BUD LT BR CN 24P|       1|BUD_LIGHT|       15.75|    288.0|         0.0546875|                 CAN| true|   WEST|WASHINGTON|RURAL_WASHINGTON|   female|  20-60k|         false|        false|50+|      full|     HS|clerical/sales/se...| white|     true|      true|   none|            true|      1|\n+---------+--------------------+--------+---------+------------+---------+------------------+--------------------+-----+-------+----------+----------------+---------+--------+--------------+-------------+---+----------+-------+--------------------+------+---------+----------+-------+----------------+-------+\nonly showing top 20 rows\n\n\n\n\n\nQ2\n\n\n\nimage.png\n\n\n\ndf = (\n    df\n    .withColumn(\"log_price_per_floz\",\n                log(df['price_floz']) )\n    .withColumn(\"log_beer_floz\",\n                log(df['beer_floz']) )\n)\n\ndtrain, dtest = df.randomSplit([0.67, 0.33], seed = 1234)\n\n\n\nQ3 Model 1\n\n\n\nimage.png\n\n\n\ndtrain, dtest = df.randomSplit([0.67, 0.33], seed = 1234)\n\n\ndummy_cols_brand, ref_category_brand = add_dummy_variables('brand', 0)\ndummy_cols_market, ref_category_market = add_dummy_variables('market', 5)\n\nReference category (dummy omitted): BUD_LIGHT\nReference category (dummy omitted): BUFFALO-ROCHESTER\n\n\n\nprint(dtrain.columns)\n\n['household', 'X_purchase_desc', 'quantity', 'brand', 'dollar_spent', 'beer_floz', 'price_floz', 'container', 'promo', 'region', 'state', 'market', 'buyertype', 'income', 'childrenUnder6', 'children6to17', 'age', 'employment', 'degree', 'occupation', 'ethnic', 'microwave', 'dishwasher', 'tvcable', 'singlefamilyhome', 'npeople', 'log_price_per_floz', 'log_beer_floz', 'brand_BUD_LIGHT', 'brand_BUSCH_LIGHT', 'brand_COORS_LIGHT', 'brand_MILLER_LITE', 'brand_NATURAL_LIGHT', 'market_ALBANY', 'market_ATLANTA', 'market_BALTIMORE', 'market_BIRMINGHAM', 'market_BOSTON', 'market_BUFFALO-ROCHESTER', 'market_CHARLOTTE', 'market_CHICAGO', 'market_CINCINNATI', 'market_CLEVELAND', 'market_COLUMBUS', 'market_DALLAS', 'market_DENVER', 'market_DES_MOINES', 'market_DETROIT', 'market_EXURBAN_NJ', 'market_EXURBAN_NY', 'market_GRAND_RAPIDS', 'market_HARTFORD-NEW_HAVEN', 'market_HOUSTON', 'market_INDIANAPOLIS', 'market_JACKSONVILLE', 'market_KANSAS_CITY', 'market_LITTLE_ROCK', 'market_LOS_ANGELES', 'market_LOUISVILLE', 'market_MEMPHIS', 'market_MIAMI', 'market_MILWAUKEE', 'market_MINNEAPOLIS', 'market_NASHVILLE', 'market_NEW_ORLEANS-MOBILE', 'market_OKLAHOMA_CITY-TULSA', 'market_OMAHA', 'market_ORLANDO', 'market_PHILADELPHIA', 'market_PHOENIX', 'market_PITTSBURGH', 'market_PORTLAND', 'market_RALEIGH-DURHAM', 'market_RICHMOND', 'market_RURAL_ALABAMA', 'market_RURAL_ARKANSAS', 'market_RURAL_CALIFORNIA', 'market_RURAL_COLORADO', 'market_RURAL_FLORIDA', 'market_RURAL_GEORGIA', 'market_RURAL_IDAHO', 'market_RURAL_ILLINOIS', 'market_RURAL_INDIANA', 'market_RURAL_IOWA', 'market_RURAL_KANSAS', 'market_RURAL_KENTUCKY', 'market_RURAL_LOUISIANA', 'market_RURAL_MAINE', 'market_RURAL_MICHIGAN', 'market_RURAL_MINNESOTA', 'market_RURAL_MISSISSIPPI', 'market_RURAL_MISSOURI', 'market_RURAL_MONTANA', 'market_RURAL_NEBRASKA', 'market_RURAL_NEVADA', 'market_RURAL_NEW_HAMPSHIRE', 'market_RURAL_NEW_MEXICO', 'market_RURAL_NEW_YORK', 'market_RURAL_NORTH_CAROLINA', 'market_RURAL_NORTH_DAKOTA', 'market_RURAL_OHIO', 'market_RURAL_OKLAHOMA', 'market_RURAL_OREGON', 'market_RURAL_PENNSYLVANIA', 'market_RURAL_SOUTH_CAROLINA', 'market_RURAL_SOUTH_DAKOTA', 'market_RURAL_TENNESSEE', 'market_RURAL_TEXAS', 'market_RURAL_VERMONT', 'market_RURAL_VIRGINIA', 'market_RURAL_WASHINGTON', 'market_RURAL_WEST_VIRGINIA', 'market_RURAL_WISCONSIN', 'market_RURAL_WYOMING', 'market_SACRAMENTO', 'market_SALT_LAKE_CITY', 'market_SAN_ANTONIO', 'market_SAN_DIEGO', 'market_SAN_FRANCISCO', 'market_SEATTLE', 'market_ST_LOUIS', 'market_SURBURBAN_NJ', 'market_SURBURBAN_NY', 'market_SYRACUSE', 'market_TAMPA', 'market_URBAN_NY', 'market_WASHINGTON_DC']\n\n\n\n# assembling predictors\nconti_cols = [\"log_beer_floz\"]\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_brand +\n    dummy_cols_market\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_price_per_floz\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)\n\n# makting regression table\nprint( regression_table(model_1, assembler_1) )\n\n+-----------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| y: log_price_per_floz       |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper) |\n+-----------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| log_beer_floz               | -0.149 |           | ***  |      0.003 |   0.000 |       -0.155 |                   |       -0.144 |                   |\n| brand_BUSCH_LIGHT           | -0.274 |     0.760 | ***  |      0.002 |   0.000 |       -0.279 |             0.757 |       -0.270 |             0.764 |\n| brand_COORS_LIGHT           | -0.006 |     0.994 |  **  |      0.002 |   0.012 |       -0.010 |             0.990 |       -0.002 |             0.998 |\n| brand_MILLER_LITE           | -0.014 |     0.986 | ***  |      0.002 |   0.000 |       -0.019 |             0.981 |       -0.010 |             0.990 |\n| brand_NATURAL_LIGHT         | -0.334 |     0.716 | ***  |      0.013 |   0.000 |       -0.358 |             0.699 |       -0.309 |             0.734 |\n| market_ALBANY               |  0.031 |     1.031 |  **  |      0.010 |   0.014 |        0.011 |             1.011 |        0.051 |             1.052 |\n| market_ATLANTA              |  0.088 |     1.092 | ***  |      0.014 |   0.000 |        0.061 |             1.063 |        0.114 |             1.121 |\n| market_BALTIMORE            |  0.095 |     1.100 | ***  |      0.010 |   0.000 |        0.075 |             1.078 |        0.116 |             1.123 |\n| market_BIRMINGHAM           |  0.126 |     1.135 | ***  |      0.011 |   0.000 |        0.105 |             1.110 |        0.148 |             1.159 |\n| market_BOSTON               |  0.124 |     1.132 | ***  |      0.010 |   0.000 |        0.104 |             1.110 |        0.144 |             1.155 |\n| market_CHARLOTTE            |  0.017 |     1.017 |  *   |      0.010 |   0.095 |       -0.002 |             0.998 |        0.036 |             1.037 |\n| market_CHICAGO              | -0.000 |     1.000 |      |      0.010 |   0.994 |       -0.020 |             0.980 |        0.020 |             1.020 |\n| market_CINCINNATI           |  0.087 |     1.091 | ***  |      0.010 |   0.000 |        0.067 |             1.069 |        0.107 |             1.113 |\n| market_CLEVELAND            |  0.066 |     1.069 | ***  |      0.010 |   0.000 |        0.047 |             1.049 |        0.086 |             1.089 |\n| market_COLUMBUS             |  0.079 |     1.082 | ***  |      0.010 |   0.000 |        0.060 |             1.062 |        0.098 |             1.103 |\n| market_DALLAS               |  0.210 |     1.233 | ***  |      0.011 |   0.000 |        0.187 |             1.206 |        0.232 |             1.261 |\n| market_DENVER               |  0.127 |     1.135 | ***  |      0.012 |   0.000 |        0.104 |             1.110 |        0.150 |             1.161 |\n| market_DES_MOINES           |  0.135 |     1.145 | ***  |      0.010 |   0.000 |        0.116 |             1.123 |        0.154 |             1.167 |\n| market_DETROIT              |  0.094 |     1.098 | ***  |      0.017 |   0.000 |        0.061 |             1.063 |        0.126 |             1.134 |\n| market_EXURBAN_NJ           |  0.253 |     1.288 | ***  |      0.023 |   0.000 |        0.207 |             1.231 |        0.298 |             1.347 |\n| market_EXURBAN_NY           |  0.124 |     1.132 | ***  |      0.011 |   0.000 |        0.101 |             1.107 |        0.146 |             1.157 |\n| market_GRAND_RAPIDS         |  0.083 |     1.086 | ***  |      0.014 |   0.000 |        0.056 |             1.058 |        0.110 |             1.116 |\n| market_HARTFORD-NEW_HAVEN   |  0.137 |     1.147 | ***  |      0.010 |   0.000 |        0.118 |             1.125 |        0.157 |             1.170 |\n| market_HOUSTON              |  0.122 |     1.130 | ***  |      0.010 |   0.000 |        0.102 |             1.107 |        0.142 |             1.153 |\n| market_INDIANAPOLIS         |  0.056 |     1.058 | ***  |      0.013 |   0.000 |        0.031 |             1.032 |        0.081 |             1.084 |\n| market_JACKSONVILLE         |  0.137 |     1.147 | ***  |      0.012 |   0.000 |        0.114 |             1.121 |        0.160 |             1.174 |\n| market_KANSAS_CITY          |  0.084 |     1.087 | ***  |      0.013 |   0.000 |        0.058 |             1.060 |        0.109 |             1.115 |\n| market_LITTLE_ROCK          |  0.097 |     1.102 | ***  |      0.010 |   0.000 |        0.078 |             1.081 |        0.117 |             1.124 |\n| market_LOS_ANGELES          |  0.029 |     1.030 | ***  |      0.011 |   0.003 |        0.008 |             1.008 |        0.051 |             1.052 |\n| market_LOUISVILLE           |  0.068 |     1.070 | ***  |      0.012 |   0.000 |        0.044 |             1.045 |        0.092 |             1.096 |\n| market_MEMPHIS              |  0.135 |     1.145 | ***  |      0.009 |   0.000 |        0.117 |             1.124 |        0.154 |             1.166 |\n| market_MIAMI                |  0.120 |     1.128 | ***  |      0.012 |   0.000 |        0.098 |             1.103 |        0.143 |             1.154 |\n| market_MILWAUKEE            |  0.030 |     1.030 |  **  |      0.011 |   0.010 |        0.008 |             1.008 |        0.051 |             1.053 |\n| market_MINNEAPOLIS          |  0.131 |     1.140 | ***  |      0.011 |   0.000 |        0.110 |             1.116 |        0.152 |             1.165 |\n| market_NASHVILLE            |  0.156 |     1.168 | ***  |      0.011 |   0.000 |        0.134 |             1.143 |        0.177 |             1.194 |\n| market_NEW_ORLEANS-MOBILE   |  0.136 |     1.146 | ***  |      0.011 |   0.000 |        0.114 |             1.121 |        0.158 |             1.172 |\n| market_OKLAHOMA_CITY-TULSA  |  0.144 |     1.154 | ***  |      0.011 |   0.000 |        0.123 |             1.131 |        0.164 |             1.179 |\n| market_OMAHA                |  0.120 |     1.128 | ***  |      0.011 |   0.000 |        0.100 |             1.105 |        0.141 |             1.151 |\n| market_ORLANDO              |  0.110 |     1.117 | ***  |      0.013 |   0.000 |        0.085 |             1.089 |        0.136 |             1.145 |\n| market_PHILADELPHIA         |  0.105 |     1.111 | ***  |      0.009 |   0.000 |        0.086 |             1.090 |        0.123 |             1.131 |\n| market_PHOENIX              |  0.147 |     1.159 | ***  |      0.014 |   0.000 |        0.120 |             1.127 |        0.175 |             1.191 |\n| market_PITTSBURGH           |  0.092 |     1.096 | ***  |      0.012 |   0.000 |        0.068 |             1.071 |        0.116 |             1.123 |\n| market_PORTLAND             |  0.121 |     1.129 | ***  |      0.011 |   0.000 |        0.100 |             1.106 |        0.142 |             1.152 |\n| market_RALEIGH-DURHAM       |  0.097 |     1.102 | ***  |      0.011 |   0.000 |        0.076 |             1.079 |        0.118 |             1.125 |\n| market_RICHMOND             |  0.038 |     1.039 | ***  |      0.015 |   0.000 |        0.010 |             1.010 |        0.067 |             1.069 |\n| market_RURAL_ALABAMA        |  0.162 |     1.176 | ***  |      0.019 |   0.000 |        0.125 |             1.133 |        0.199 |             1.221 |\n| market_RURAL_ARKANSAS       |  0.181 |     1.198 | ***  |      0.011 |   0.000 |        0.159 |             1.172 |        0.202 |             1.224 |\n| market_RURAL_CALIFORNIA     |  0.042 |     1.043 | ***  |      0.048 |   0.000 |       -0.051 |             0.950 |        0.136 |             1.146 |\n| market_RURAL_COLORADO       |  0.167 |     1.181 | ***  |      0.012 |   0.000 |        0.143 |             1.153 |        0.191 |             1.210 |\n| market_RURAL_FLORIDA        |  0.072 |     1.074 | ***  |      0.013 |   0.000 |        0.046 |             1.047 |        0.097 |             1.102 |\n| market_RURAL_GEORGIA        |  0.142 |     1.153 | ***  |      0.019 |   0.000 |        0.105 |             1.111 |        0.179 |             1.196 |\n| market_RURAL_IDAHO          |  0.141 |     1.152 | ***  |      0.010 |   0.000 |        0.121 |             1.129 |        0.162 |             1.175 |\n| market_RURAL_ILLINOIS       |  0.019 |     1.019 |  *   |      0.013 |   0.063 |       -0.006 |             0.994 |        0.044 |             1.045 |\n| market_RURAL_INDIANA        |  0.079 |     1.082 | ***  |      0.011 |   0.000 |        0.058 |             1.060 |        0.099 |             1.105 |\n| market_RURAL_IOWA           |  0.069 |     1.072 | ***  |      0.018 |   0.000 |        0.034 |             1.035 |        0.104 |             1.110 |\n| market_RURAL_KANSAS         |  0.134 |     1.143 | ***  |      0.016 |   0.000 |        0.102 |             1.108 |        0.166 |             1.180 |\n| market_RURAL_KENTUCKY       |  0.152 |     1.165 | ***  |      0.013 |   0.000 |        0.126 |             1.135 |        0.178 |             1.195 |\n| market_RURAL_LOUISIANA      |  0.060 |     1.062 | ***  |      0.014 |   0.000 |        0.033 |             1.033 |        0.088 |             1.092 |\n| market_RURAL_MAINE          |  0.083 |     1.087 | ***  |      0.011 |   0.000 |        0.061 |             1.063 |        0.105 |             1.111 |\n| market_RURAL_MICHIGAN       |  0.081 |     1.085 | ***  |      0.020 |   0.000 |        0.042 |             1.043 |        0.120 |             1.128 |\n| market_RURAL_MINNESOTA      |  0.180 |     1.197 | ***  |      0.014 |   0.000 |        0.153 |             1.165 |        0.207 |             1.230 |\n| market_RURAL_MISSISSIPPI    |  0.037 |     1.037 | ***  |      0.012 |   0.008 |        0.014 |             1.014 |        0.060 |             1.062 |\n| market_RURAL_MISSOURI       |  0.114 |     1.121 | ***  |      0.014 |   0.000 |        0.087 |             1.091 |        0.142 |             1.152 |\n| market_RURAL_MONTANA        |  0.127 |     1.135 | ***  |      0.022 |   0.000 |        0.084 |             1.088 |        0.170 |             1.185 |\n| market_RURAL_NEBRASKA       |  0.137 |     1.147 | ***  |      0.012 |   0.000 |        0.113 |             1.120 |        0.161 |             1.175 |\n| market_RURAL_NEVADA         |  0.049 |     1.050 | ***  |      0.042 |   0.000 |       -0.034 |             0.967 |        0.131 |             1.140 |\n| market_RURAL_NEW_HAMPSHIRE  |  0.068 |     1.070 |      |      0.013 |   0.107 |        0.042 |             1.043 |        0.093 |             1.098 |\n| market_RURAL_NEW_MEXICO     |  0.173 |     1.189 | ***  |      0.057 |   0.000 |        0.061 |             1.063 |        0.285 |             1.330 |\n| market_RURAL_NEW_YORK       | -0.018 |     0.982 |      |      0.011 |   0.753 |       -0.039 |             0.961 |        0.003 |             1.003 |\n| market_RURAL_NORTH_CAROLINA |  0.011 |     1.011 |      |      0.020 |   0.309 |       -0.029 |             0.972 |        0.051 |             1.052 |\n| market_RURAL_NORTH_DAKOTA   |  0.223 |     1.249 | ***  |      0.015 |   0.000 |        0.193 |             1.212 |        0.253 |             1.288 |\n| market_RURAL_OHIO           |  0.097 |     1.102 | ***  |      0.031 |   0.000 |        0.036 |             1.037 |        0.158 |             1.171 |\n| market_RURAL_OKLAHOMA       |  0.119 |     1.126 | ***  |      0.034 |   0.000 |        0.053 |             1.054 |        0.185 |             1.203 |\n| market_RURAL_OREGON         |  0.005 |     1.005 |      |      0.015 |   0.890 |       -0.025 |             0.976 |        0.034 |             1.034 |\n| market_RURAL_PENNSYLVANIA   |  0.132 |     1.141 | ***  |      0.010 |   0.000 |        0.112 |             1.119 |        0.152 |             1.164 |\n| market_RURAL_SOUTH_CAROLINA |  0.055 |     1.056 | ***  |      0.018 |   0.000 |        0.019 |             1.019 |        0.091 |             1.095 |\n| market_RURAL_SOUTH_DAKOTA   |  0.065 |     1.067 | ***  |      0.013 |   0.000 |        0.039 |             1.039 |        0.091 |             1.095 |\n| market_RURAL_TENNESSEE      |  0.173 |     1.189 | ***  |      0.010 |   0.000 |        0.153 |             1.166 |        0.192 |             1.212 |\n| market_RURAL_TEXAS          |  0.173 |     1.188 | ***  |      0.020 |   0.000 |        0.133 |             1.142 |        0.212 |             1.236 |\n| market_RURAL_VERMONT        |  0.101 |     1.107 | ***  |      0.017 |   0.000 |        0.068 |             1.070 |        0.135 |             1.145 |\n| market_RURAL_VIRGINIA       |  0.025 |     1.025 |      |      0.014 |   0.155 |       -0.003 |             0.997 |        0.052 |             1.053 |\n| market_RURAL_WASHINGTON     |  0.096 |     1.101 | ***  |      0.016 |   0.000 |        0.066 |             1.068 |        0.127 |             1.135 |\n| market_RURAL_WEST_VIRGINIA  | -0.032 |     0.968 |  **  |      0.010 |   0.039 |       -0.052 |             0.949 |       -0.012 |             0.988 |\n| market_RURAL_WISCONSIN      |  0.040 |     1.041 | ***  |      0.036 |   0.000 |       -0.030 |             0.970 |        0.110 |             1.116 |\n| market_RURAL_WYOMING        |  0.198 |     1.219 | ***  |      0.011 |   0.000 |        0.177 |             1.194 |        0.219 |             1.245 |\n| market_SACRAMENTO           |  0.032 |     1.032 | ***  |      0.014 |   0.003 |        0.004 |             1.004 |        0.060 |             1.062 |\n| market_SALT_LAKE_CITY       |  0.113 |     1.119 | ***  |      0.009 |   0.000 |        0.094 |             1.099 |        0.131 |             1.140 |\n| market_SAN_ANTONIO          |  0.143 |     1.154 | ***  |      0.012 |   0.000 |        0.120 |             1.127 |        0.166 |             1.180 |\n| market_SAN_DIEGO            |  0.009 |     1.009 |      |      0.011 |   0.451 |       -0.013 |             0.987 |        0.031 |             1.031 |\n| market_SAN_FRANCISCO        |  0.065 |     1.067 | ***  |      0.011 |   0.000 |        0.043 |             1.044 |        0.086 |             1.090 |\n| market_SEATTLE              |  0.114 |     1.121 | ***  |      0.010 |   0.000 |        0.094 |             1.099 |        0.134 |             1.144 |\n| market_ST_LOUIS             |  0.046 |     1.047 | ***  |      0.013 |   0.000 |        0.020 |             1.020 |        0.073 |             1.075 |\n| market_SURBURBAN_NJ         | -0.022 |     0.979 |      |      0.013 |   0.109 |       -0.046 |             0.955 |        0.003 |             1.003 |\n| market_SURBURBAN_NY         |  0.122 |     1.130 | ***  |      0.015 |   0.000 |        0.093 |             1.097 |        0.151 |             1.163 |\n| market_SYRACUSE             | -0.051 |     0.950 | ***  |      0.009 |   0.001 |       -0.069 |             0.933 |       -0.033 |             0.967 |\n| market_TAMPA                |  0.115 |     1.122 | ***  |      0.011 |   0.000 |        0.093 |             1.097 |        0.137 |             1.147 |\n| market_URBAN_NY             |  0.163 |     1.177 | ***  |      0.011 |   0.000 |        0.141 |             1.151 |        0.184 |             1.202 |\n| market_WASHINGTON_DC        |  0.104 |     1.110 | ***  |      0.011 |   0.000 |        0.084 |             1.087 |        0.125 |             1.133 |\n| Intercept                   | -2.115 |           | ***  |      0.001 |         |       -2.118 |                   |       -2.113 |                   |\n--------------------------------------------------------------------------------------------------------------------------------------------------------\n| Observations                | 48,153 |           |      |            |         |              |                   |              |                   |\n| R²                          |  0.543 |           |      |            |         |              |                   |              |                   |\n| RMSE                        |  0.170 |           |      |            |         |              |                   |              |                   |\n+-----------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n\n\n\n# The model examines the relationship between different predictors and the log-transformed price per fluid ounce.\n# One interesting thing we see right away is BUSCH_LIGHT has a negative relationship with the price per fluid ounce  (0.760).\n# We also see that The variable log(Beer_price) shows a significant positive relationship with the\n# log-transformed price per fluid ounce, indicating that higher beer prices lead to a higher price per fluid ounce.\n\n\n\nQ3 Model 2\n\n\n\nimage.png\n\n\n\ninteraction_cols_beer_brand = add_interaction_terms(dummy_cols_brand, ['log_beer_floz'])\n\n\n# assembling predictors\nconti_cols = [\"log_beer_floz\"]\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_brand +\n    dummy_cols_market +\n    interaction_cols_beer_brand\n)\n\nassembler_2 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_2 = assembler_2.transform(dtrain)\ndtest_2  = assembler_2.transform(dtest)\n\n# training model\nmodel_2 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_price_per_floz\")\n    .fit(dtrain_2)\n)\n\n# making prediction\ndtest_2 = model_2.transform(dtest_2)\n\n# making regression table\nprint( regression_table(model_2, assembler_2) )\n\n+-------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| y: log_price_per_floz               |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper) |\n+-------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| log_beer_floz                       | -0.155 |           | ***  |      0.022 |   0.000 |       -0.199 |                   |       -0.112 |                   |\n| brand_BUSCH_LIGHT                   | -0.233 |     0.792 | ***  |      0.019 |   0.000 |       -0.269 |             0.764 |       -0.196 |             0.822 |\n| brand_COORS_LIGHT                   |  0.021 |     1.021 |      |      0.017 |   0.266 |       -0.012 |             0.988 |        0.054 |             1.055 |\n| brand_MILLER_LITE                   |  0.064 |     1.066 | ***  |      0.018 |   0.000 |        0.030 |             1.030 |        0.098 |             1.103 |\n| brand_NATURAL_LIGHT                 | -0.633 |     0.531 | ***  |      0.012 |   0.000 |       -0.657 |             0.518 |       -0.608 |             0.544 |\n| market_ALBANY                       |  0.033 |     1.033 | ***  |      0.010 |   0.009 |        0.013 |             1.013 |        0.053 |             1.054 |\n| market_ATLANTA                      |  0.086 |     1.090 | ***  |      0.013 |   0.000 |        0.060 |             1.062 |        0.113 |             1.119 |\n| market_BALTIMORE                    |  0.099 |     1.104 | ***  |      0.010 |   0.000 |        0.079 |             1.082 |        0.119 |             1.127 |\n| market_BIRMINGHAM                   |  0.133 |     1.142 | ***  |      0.011 |   0.000 |        0.112 |             1.118 |        0.155 |             1.167 |\n| market_BOSTON                       |  0.124 |     1.132 | ***  |      0.010 |   0.000 |        0.104 |             1.109 |        0.144 |             1.154 |\n| market_CHARLOTTE                    |  0.013 |     1.013 |      |      0.010 |   0.210 |       -0.006 |             0.994 |        0.032 |             1.032 |\n| market_CHICAGO                      | -0.006 |     0.994 |      |      0.010 |   0.544 |       -0.026 |             0.974 |        0.014 |             1.014 |\n| market_CINCINNATI                   |  0.082 |     1.085 | ***  |      0.010 |   0.000 |        0.062 |             1.063 |        0.102 |             1.107 |\n| market_CLEVELAND                    |  0.061 |     1.063 | ***  |      0.010 |   0.000 |        0.042 |             1.043 |        0.080 |             1.084 |\n| market_COLUMBUS                     |  0.075 |     1.078 | ***  |      0.010 |   0.000 |        0.057 |             1.058 |        0.094 |             1.099 |\n| market_DALLAS                       |  0.219 |     1.245 | ***  |      0.011 |   0.000 |        0.197 |             1.218 |        0.241 |             1.273 |\n| market_DENVER                       |  0.124 |     1.132 | ***  |      0.012 |   0.000 |        0.101 |             1.107 |        0.147 |             1.158 |\n| market_DES_MOINES                   |  0.132 |     1.141 | ***  |      0.010 |   0.000 |        0.113 |             1.119 |        0.151 |             1.163 |\n| market_DETROIT                      |  0.089 |     1.093 | ***  |      0.017 |   0.000 |        0.056 |             1.058 |        0.121 |             1.129 |\n| market_EXURBAN_NJ                   |  0.246 |     1.279 | ***  |      0.023 |   0.000 |        0.201 |             1.223 |        0.291 |             1.338 |\n| market_EXURBAN_NY                   |  0.120 |     1.127 | ***  |      0.011 |   0.000 |        0.097 |             1.102 |        0.142 |             1.152 |\n| market_GRAND_RAPIDS                 |  0.079 |     1.082 | ***  |      0.014 |   0.000 |        0.052 |             1.053 |        0.105 |             1.111 |\n| market_HARTFORD-NEW_HAVEN           |  0.134 |     1.144 | ***  |      0.010 |   0.000 |        0.115 |             1.122 |        0.154 |             1.166 |\n| market_HOUSTON                      |  0.119 |     1.126 | ***  |      0.010 |   0.000 |        0.099 |             1.104 |        0.139 |             1.149 |\n| market_INDIANAPOLIS                 |  0.055 |     1.056 | ***  |      0.012 |   0.000 |        0.030 |             1.031 |        0.079 |             1.082 |\n| market_JACKSONVILLE                 |  0.130 |     1.139 | ***  |      0.012 |   0.000 |        0.107 |             1.113 |        0.153 |             1.166 |\n| market_KANSAS_CITY                  |  0.078 |     1.081 | ***  |      0.013 |   0.000 |        0.052 |             1.054 |        0.103 |             1.109 |\n| market_LITTLE_ROCK                  |  0.093 |     1.097 | ***  |      0.010 |   0.000 |        0.074 |             1.076 |        0.112 |             1.119 |\n| market_LOS_ANGELES                  |  0.023 |     1.023 |  **  |      0.011 |   0.020 |        0.001 |             1.001 |        0.045 |             1.046 |\n| market_LOUISVILLE                   |  0.063 |     1.065 | ***  |      0.012 |   0.000 |        0.039 |             1.040 |        0.087 |             1.091 |\n| market_MEMPHIS                      |  0.134 |     1.144 | ***  |      0.009 |   0.000 |        0.116 |             1.123 |        0.153 |             1.165 |\n| market_MIAMI                        |  0.119 |     1.126 | ***  |      0.012 |   0.000 |        0.096 |             1.101 |        0.141 |             1.152 |\n| market_MILWAUKEE                    |  0.029 |     1.029 |  **  |      0.011 |   0.013 |        0.007 |             1.007 |        0.050 |             1.052 |\n| market_MINNEAPOLIS                  |  0.132 |     1.141 | ***  |      0.011 |   0.000 |        0.111 |             1.118 |        0.153 |             1.166 |\n| market_NASHVILLE                    |  0.155 |     1.168 | ***  |      0.011 |   0.000 |        0.133 |             1.143 |        0.176 |             1.193 |\n| market_NEW_ORLEANS-MOBILE           |  0.126 |     1.135 | ***  |      0.011 |   0.000 |        0.105 |             1.110 |        0.148 |             1.160 |\n| market_OKLAHOMA_CITY-TULSA          |  0.140 |     1.151 | ***  |      0.011 |   0.000 |        0.120 |             1.127 |        0.161 |             1.175 |\n| market_OMAHA                        |  0.117 |     1.125 | ***  |      0.010 |   0.000 |        0.097 |             1.102 |        0.138 |             1.148 |\n| market_ORLANDO                      |  0.108 |     1.114 | ***  |      0.013 |   0.000 |        0.082 |             1.086 |        0.133 |             1.142 |\n| market_PHILADELPHIA                 |  0.105 |     1.110 | ***  |      0.009 |   0.000 |        0.086 |             1.090 |        0.123 |             1.131 |\n| market_PHOENIX                      |  0.150 |     1.162 | ***  |      0.014 |   0.000 |        0.122 |             1.130 |        0.177 |             1.194 |\n| market_PITTSBURGH                   |  0.090 |     1.094 | ***  |      0.012 |   0.000 |        0.066 |             1.068 |        0.114 |             1.120 |\n| market_PORTLAND                     |  0.119 |     1.126 | ***  |      0.010 |   0.000 |        0.098 |             1.103 |        0.139 |             1.149 |\n| market_RALEIGH-DURHAM               |  0.096 |     1.101 | ***  |      0.011 |   0.000 |        0.075 |             1.078 |        0.117 |             1.124 |\n| market_RICHMOND                     |  0.035 |     1.036 | ***  |      0.014 |   0.001 |        0.007 |             1.007 |        0.063 |             1.065 |\n| market_RURAL_ALABAMA                |  0.162 |     1.175 | ***  |      0.019 |   0.000 |        0.125 |             1.133 |        0.199 |             1.220 |\n| market_RURAL_ARKANSAS               |  0.181 |     1.199 | ***  |      0.011 |   0.000 |        0.160 |             1.173 |        0.203 |             1.225 |\n| market_RURAL_CALIFORNIA             |  0.038 |     1.039 | ***  |      0.048 |   0.000 |       -0.055 |             0.947 |        0.132 |             1.141 |\n| market_RURAL_COLORADO               |  0.164 |     1.178 | ***  |      0.012 |   0.001 |        0.140 |             1.150 |        0.188 |             1.207 |\n| market_RURAL_FLORIDA                |  0.063 |     1.065 | ***  |      0.013 |   0.000 |        0.037 |             1.038 |        0.088 |             1.092 |\n| market_RURAL_GEORGIA                |  0.140 |     1.150 | ***  |      0.019 |   0.000 |        0.104 |             1.109 |        0.177 |             1.193 |\n| market_RURAL_IDAHO                  |  0.133 |     1.142 | ***  |      0.010 |   0.000 |        0.113 |             1.120 |        0.153 |             1.166 |\n| market_RURAL_ILLINOIS               |  0.017 |     1.017 |  *   |      0.013 |   0.099 |       -0.008 |             0.992 |        0.042 |             1.043 |\n| market_RURAL_INDIANA                |  0.080 |     1.083 | ***  |      0.011 |   0.000 |        0.059 |             1.061 |        0.100 |             1.106 |\n| market_RURAL_IOWA                   |  0.065 |     1.067 | ***  |      0.018 |   0.000 |        0.030 |             1.031 |        0.100 |             1.105 |\n| market_RURAL_KANSAS                 |  0.134 |     1.143 | ***  |      0.016 |   0.000 |        0.102 |             1.107 |        0.165 |             1.180 |\n| market_RURAL_KENTUCKY               |  0.151 |     1.163 | ***  |      0.013 |   0.000 |        0.125 |             1.133 |        0.177 |             1.193 |\n| market_RURAL_LOUISIANA              |  0.056 |     1.057 | ***  |      0.014 |   0.000 |        0.028 |             1.029 |        0.083 |             1.086 |\n| market_RURAL_MAINE                  |  0.079 |     1.083 | ***  |      0.011 |   0.000 |        0.057 |             1.059 |        0.102 |             1.107 |\n| market_RURAL_MICHIGAN               |  0.078 |     1.081 | ***  |      0.020 |   0.000 |        0.039 |             1.039 |        0.116 |             1.123 |\n| market_RURAL_MINNESOTA              |  0.183 |     1.201 | ***  |      0.014 |   0.000 |        0.156 |             1.169 |        0.210 |             1.233 |\n| market_RURAL_MISSISSIPPI            |  0.031 |     1.032 |  **  |      0.012 |   0.024 |        0.008 |             1.008 |        0.054 |             1.056 |\n| market_RURAL_MISSOURI               |  0.114 |     1.120 | ***  |      0.014 |   0.000 |        0.086 |             1.090 |        0.141 |             1.151 |\n| market_RURAL_MONTANA                |  0.124 |     1.132 | ***  |      0.022 |   0.000 |        0.081 |             1.084 |        0.166 |             1.181 |\n| market_RURAL_NEBRASKA               |  0.136 |     1.146 | ***  |      0.012 |   0.000 |        0.113 |             1.119 |        0.160 |             1.173 |\n| market_RURAL_NEVADA                 |  0.048 |     1.049 | ***  |      0.042 |   0.000 |       -0.034 |             0.967 |        0.130 |             1.139 |\n| market_RURAL_NEW_HAMPSHIRE          |  0.053 |     1.055 |      |      0.013 |   0.201 |        0.028 |             1.028 |        0.079 |             1.082 |\n| market_RURAL_NEW_MEXICO             |  0.168 |     1.183 | ***  |      0.057 |   0.000 |        0.056 |             1.058 |        0.280 |             1.323 |\n| market_RURAL_NEW_YORK               | -0.015 |     0.985 |      |      0.011 |   0.795 |       -0.036 |             0.964 |        0.007 |             1.007 |\n| market_RURAL_NORTH_CAROLINA         |  0.040 |     1.041 | ***  |      0.020 |   0.000 |        0.001 |             1.001 |        0.080 |             1.083 |\n| market_RURAL_NORTH_DAKOTA           |  0.222 |     1.248 | ***  |      0.015 |   0.000 |        0.192 |             1.211 |        0.252 |             1.286 |\n| market_RURAL_OHIO                   |  0.095 |     1.099 | ***  |      0.031 |   0.000 |        0.034 |             1.034 |        0.155 |             1.168 |\n| market_RURAL_OKLAHOMA               |  0.119 |     1.126 | ***  |      0.034 |   0.000 |        0.053 |             1.055 |        0.185 |             1.203 |\n| market_RURAL_OREGON                 | -0.000 |     1.000 |      |      0.015 |   0.992 |       -0.029 |             0.971 |        0.029 |             1.029 |\n| market_RURAL_PENNSYLVANIA           |  0.133 |     1.142 | ***  |      0.010 |   0.000 |        0.113 |             1.119 |        0.153 |             1.165 |\n| market_RURAL_SOUTH_CAROLINA         |  0.053 |     1.054 | ***  |      0.018 |   0.000 |        0.017 |             1.017 |        0.089 |             1.093 |\n| market_RURAL_SOUTH_DAKOTA           |  0.062 |     1.064 | ***  |      0.013 |   0.001 |        0.036 |             1.037 |        0.088 |             1.092 |\n| market_RURAL_TENNESSEE              |  0.173 |     1.189 | ***  |      0.010 |   0.000 |        0.154 |             1.166 |        0.192 |             1.212 |\n| market_RURAL_TEXAS                  |  0.171 |     1.186 | ***  |      0.020 |   0.000 |        0.131 |             1.140 |        0.210 |             1.234 |\n| market_RURAL_VERMONT                |  0.089 |     1.093 | ***  |      0.017 |   0.000 |        0.056 |             1.057 |        0.123 |             1.131 |\n| market_RURAL_VIRGINIA               |  0.023 |     1.024 |      |      0.014 |   0.173 |       -0.004 |             0.996 |        0.051 |             1.052 |\n| market_RURAL_WASHINGTON             |  0.094 |     1.099 | ***  |      0.016 |   0.000 |        0.064 |             1.066 |        0.125 |             1.133 |\n| market_RURAL_WEST_VIRGINIA          | -0.037 |     0.964 |  **  |      0.010 |   0.018 |       -0.057 |             0.945 |       -0.017 |             0.984 |\n| market_RURAL_WISCONSIN              |  0.038 |     1.038 | ***  |      0.035 |   0.000 |       -0.032 |             0.969 |        0.107 |             1.113 |\n| market_RURAL_WYOMING                |  0.197 |     1.218 | ***  |      0.011 |   0.000 |        0.176 |             1.193 |        0.218 |             1.244 |\n| market_SACRAMENTO                   |  0.030 |     1.031 | ***  |      0.014 |   0.004 |        0.003 |             1.003 |        0.058 |             1.060 |\n| market_SALT_LAKE_CITY               |  0.107 |     1.112 | ***  |      0.009 |   0.000 |        0.088 |             1.092 |        0.125 |             1.133 |\n| market_SAN_ANTONIO                  |  0.138 |     1.148 | ***  |      0.012 |   0.000 |        0.115 |             1.122 |        0.161 |             1.175 |\n| market_SAN_DIEGO                    |  0.008 |     1.008 |      |      0.011 |   0.493 |       -0.014 |             0.987 |        0.030 |             1.030 |\n| market_SAN_FRANCISCO                |  0.062 |     1.064 | ***  |      0.011 |   0.000 |        0.040 |             1.041 |        0.083 |             1.087 |\n| market_SEATTLE                      |  0.104 |     1.110 | ***  |      0.010 |   0.000 |        0.084 |             1.088 |        0.124 |             1.132 |\n| market_ST_LOUIS                     |  0.042 |     1.043 | ***  |      0.013 |   0.000 |        0.016 |             1.016 |        0.068 |             1.071 |\n| market_SURBURBAN_NJ                 | -0.024 |     0.976 |  *   |      0.013 |   0.073 |       -0.049 |             0.953 |        0.001 |             1.001 |\n| market_SURBURBAN_NY                 |  0.118 |     1.125 | ***  |      0.015 |   0.000 |        0.089 |             1.093 |        0.147 |             1.158 |\n| market_SYRACUSE                     | -0.056 |     0.945 | ***  |      0.009 |   0.000 |       -0.074 |             0.928 |       -0.038 |             0.962 |\n| market_TAMPA                        |  0.111 |     1.117 | ***  |      0.011 |   0.000 |        0.089 |             1.093 |        0.133 |             1.142 |\n| market_URBAN_NY                     |  0.162 |     1.176 | ***  |      0.011 |   0.000 |        0.141 |             1.151 |        0.184 |             1.202 |\n| market_WASHINGTON_DC                |  0.097 |     1.102 | ***  |      0.004 |   0.000 |        0.089 |             1.093 |        0.105 |             1.111 |\n| brand_BUSCH_LIGHT_*_log_beer_floz   | -0.007 |           |  *   |      0.003 |   0.070 |       -0.014 |                   |       -0.000 |                   |\n| brand_COORS_LIGHT_*_log_beer_floz   | -0.005 |           |      |      0.003 |   0.141 |       -0.011 |                   |        0.001 |                   |\n| brand_MILLER_LITE_*_log_beer_floz   | -0.015 |           | ***  |      0.003 |   0.000 |       -0.021 |                   |       -0.008 |                   |\n| brand_NATURAL_LIGHT_*_log_beer_floz |  0.056 |           | ***  |      0.014 |   0.000 |        0.028 |                   |        0.083 |                   |\n| Intercept                           | -2.079 |           | ***  |      0.002 |         |       -2.083 |                   |       -2.075 |                   |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Observations                        | 48,153 |           |      |            |         |              |                   |              |                   |\n| R²                                  |  0.548 |           |      |            |         |              |                   |              |                   |\n| RMSE                                |  0.169 |           |      |            |         |              |                   |              |                   |\n+-------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n\n\n\n# With the 2nd model, we see Log-Beer FL Oz has a significant negative impact on the log price per fluid ounce (β = -0.155, p &lt; 0.001),\n# suggesting that as the beer quantity increases, the price decreases. We also see some brand like Busch Light went from 0.274 to 0.233 which\n# suggest that Busch Light in the model means that the price effect of Busch Light is now slightly smaller.\n\n\n\nQ3 Model 3\n\n\n\nimage.png\n\n\n\ndummy_cols_promo, ref_category_brand = add_dummy_variables('promo', 0)\n\nReference category (dummy omitted): False\n\n\n\ninteraction_cols_beer_brand_promo = add_interaction_terms(dummy_cols_brand, dummy_cols_promo, ['log_beer_floz'])\n\n\n# assembling predictors\nconti_cols = [\"log_beer_floz\"]\n\nassembler_predictors = (\n    conti_cols +\n    dummy_cols_brand +\n    dummy_cols_market +\n   # interaction_cols_beer_brand +\n    interaction_cols_beer_brand_promo\n)\n\nassembler_3 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_3 = assembler_3.transform(dtrain)\ndtest_3  = assembler_3.transform(dtest)\n\n# training model\nmodel_3 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"log_price_per_floz\")\n    .fit(dtrain_3)\n)\n\n# making prediction\ndtest_3 = model_3.transform(dtest_3)\n\n# makting regression table\nprint( regression_table(model_3, assembler_3) )\n\n+--------------------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| y: log_price_per_floz                            |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper) |\n+--------------------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n| log_beer_floz                                    | -0.151 |           | ***  |      0.023 |   0.000 |       -0.196 |                   |       -0.106 |                   |\n| brand_BUSCH_LIGHT                                | -0.196 |     0.822 | ***  |      0.019 |   0.000 |       -0.234 |             0.791 |       -0.158 |             0.854 |\n| brand_COORS_LIGHT                                |  0.033 |     1.033 |  *   |      0.018 |   0.090 |       -0.002 |             0.998 |        0.067 |             1.070 |\n| brand_MILLER_LITE                                |  0.091 |     1.095 | ***  |      0.018 |   0.000 |        0.055 |             1.056 |        0.127 |             1.135 |\n| brand_NATURAL_LIGHT                              | -0.554 |     0.575 | ***  |      0.012 |   0.000 |       -0.578 |             0.561 |       -0.530 |             0.589 |\n| market_ALBANY                                    |  0.025 |     1.025 |  **  |      0.010 |   0.044 |        0.005 |             1.005 |        0.045 |             1.046 |\n| market_ATLANTA                                   |  0.083 |     1.087 | ***  |      0.013 |   0.000 |        0.057 |             1.058 |        0.109 |             1.116 |\n| market_BALTIMORE                                 |  0.090 |     1.094 | ***  |      0.010 |   0.000 |        0.070 |             1.072 |        0.110 |             1.117 |\n| market_BIRMINGHAM                                |  0.128 |     1.137 | ***  |      0.011 |   0.000 |        0.107 |             1.113 |        0.150 |             1.161 |\n| market_BOSTON                                    |  0.121 |     1.129 | ***  |      0.010 |   0.000 |        0.101 |             1.107 |        0.141 |             1.152 |\n| market_CHARLOTTE                                 |  0.023 |     1.023 |  **  |      0.010 |   0.023 |        0.004 |             1.004 |        0.042 |             1.043 |\n| market_CHICAGO                                   |  0.001 |     1.001 |      |      0.010 |   0.917 |       -0.019 |             0.981 |        0.021 |             1.021 |\n| market_CINCINNATI                                |  0.080 |     1.084 | ***  |      0.010 |   0.000 |        0.060 |             1.062 |        0.100 |             1.106 |\n| market_CLEVELAND                                 |  0.059 |     1.061 | ***  |      0.010 |   0.000 |        0.040 |             1.041 |        0.078 |             1.081 |\n| market_COLUMBUS                                  |  0.075 |     1.078 | ***  |      0.010 |   0.000 |        0.057 |             1.058 |        0.094 |             1.098 |\n| market_DALLAS                                    |  0.224 |     1.252 | ***  |      0.011 |   0.000 |        0.202 |             1.224 |        0.246 |             1.279 |\n| market_DENVER                                    |  0.135 |     1.145 | ***  |      0.011 |   0.000 |        0.113 |             1.120 |        0.158 |             1.171 |\n| market_DES_MOINES                                |  0.126 |     1.134 | ***  |      0.010 |   0.000 |        0.107 |             1.113 |        0.145 |             1.156 |\n| market_DETROIT                                   |  0.092 |     1.096 | ***  |      0.017 |   0.000 |        0.059 |             1.061 |        0.124 |             1.132 |\n| market_EXURBAN_NJ                                |  0.238 |     1.269 | ***  |      0.023 |   0.000 |        0.193 |             1.213 |        0.283 |             1.327 |\n| market_EXURBAN_NY                                |  0.116 |     1.123 | ***  |      0.011 |   0.000 |        0.094 |             1.098 |        0.138 |             1.148 |\n| market_GRAND_RAPIDS                              |  0.078 |     1.081 | ***  |      0.013 |   0.000 |        0.051 |             1.053 |        0.104 |             1.110 |\n| market_HARTFORD-NEW_HAVEN                        |  0.133 |     1.143 | ***  |      0.010 |   0.000 |        0.114 |             1.121 |        0.153 |             1.165 |\n| market_HOUSTON                                   |  0.123 |     1.130 | ***  |      0.010 |   0.000 |        0.103 |             1.108 |        0.143 |             1.153 |\n| market_INDIANAPOLIS                              |  0.057 |     1.059 | ***  |      0.012 |   0.000 |        0.033 |             1.033 |        0.081 |             1.085 |\n| market_JACKSONVILLE                              |  0.132 |     1.141 | ***  |      0.012 |   0.000 |        0.109 |             1.115 |        0.154 |             1.167 |\n| market_KANSAS_CITY                               |  0.074 |     1.077 | ***  |      0.013 |   0.000 |        0.049 |             1.050 |        0.100 |             1.105 |\n| market_LITTLE_ROCK                               |  0.090 |     1.094 | ***  |      0.010 |   0.000 |        0.070 |             1.073 |        0.109 |             1.115 |\n| market_LOS_ANGELES                               |  0.032 |     1.033 | ***  |      0.011 |   0.001 |        0.011 |             1.011 |        0.054 |             1.055 |\n| market_LOUISVILLE                                |  0.068 |     1.070 | ***  |      0.012 |   0.000 |        0.044 |             1.045 |        0.091 |             1.096 |\n| market_MEMPHIS                                   |  0.128 |     1.137 | ***  |      0.009 |   0.000 |        0.110 |             1.117 |        0.147 |             1.158 |\n| market_MIAMI                                     |  0.120 |     1.128 | ***  |      0.011 |   0.000 |        0.098 |             1.103 |        0.143 |             1.153 |\n| market_MILWAUKEE                                 |  0.031 |     1.031 | ***  |      0.011 |   0.007 |        0.009 |             1.009 |        0.052 |             1.053 |\n| market_MINNEAPOLIS                               |  0.129 |     1.138 | ***  |      0.011 |   0.000 |        0.108 |             1.114 |        0.150 |             1.162 |\n| market_NASHVILLE                                 |  0.154 |     1.166 | ***  |      0.011 |   0.000 |        0.132 |             1.141 |        0.175 |             1.191 |\n| market_NEW_ORLEANS-MOBILE                        |  0.122 |     1.129 | ***  |      0.011 |   0.000 |        0.100 |             1.105 |        0.143 |             1.154 |\n| market_OKLAHOMA_CITY-TULSA                       |  0.134 |     1.144 | ***  |      0.011 |   0.000 |        0.114 |             1.120 |        0.155 |             1.167 |\n| market_OMAHA                                     |  0.120 |     1.128 | ***  |      0.010 |   0.000 |        0.100 |             1.105 |        0.141 |             1.151 |\n| market_ORLANDO                                   |  0.112 |     1.118 | ***  |      0.013 |   0.000 |        0.087 |             1.091 |        0.137 |             1.147 |\n| market_PHILADELPHIA                              |  0.094 |     1.099 | ***  |      0.009 |   0.000 |        0.076 |             1.079 |        0.113 |             1.119 |\n| market_PHOENIX                                   |  0.159 |     1.172 | ***  |      0.014 |   0.000 |        0.131 |             1.141 |        0.186 |             1.205 |\n| market_PITTSBURGH                                |  0.085 |     1.089 | ***  |      0.012 |   0.000 |        0.062 |             1.063 |        0.109 |             1.115 |\n| market_PORTLAND                                  |  0.122 |     1.129 | ***  |      0.010 |   0.000 |        0.101 |             1.107 |        0.142 |             1.153 |\n| market_RALEIGH-DURHAM                            |  0.092 |     1.097 | ***  |      0.010 |   0.000 |        0.072 |             1.074 |        0.113 |             1.119 |\n| market_RICHMOND                                  |  0.030 |     1.031 | ***  |      0.014 |   0.004 |        0.002 |             1.002 |        0.058 |             1.060 |\n| market_RURAL_ALABAMA                             |  0.158 |     1.171 | ***  |      0.019 |   0.000 |        0.121 |             1.129 |        0.195 |             1.215 |\n| market_RURAL_ARKANSAS                            |  0.175 |     1.191 | ***  |      0.011 |   0.000 |        0.153 |             1.166 |        0.196 |             1.217 |\n| market_RURAL_CALIFORNIA                          |  0.042 |     1.043 | ***  |      0.047 |   0.000 |       -0.050 |             0.951 |        0.135 |             1.144 |\n| market_RURAL_COLORADO                            |  0.171 |     1.187 | ***  |      0.012 |   0.000 |        0.147 |             1.159 |        0.195 |             1.215 |\n| market_RURAL_FLORIDA                             |  0.062 |     1.064 | ***  |      0.013 |   0.000 |        0.037 |             1.038 |        0.087 |             1.091 |\n| market_RURAL_GEORGIA                             |  0.135 |     1.145 | ***  |      0.018 |   0.000 |        0.099 |             1.104 |        0.171 |             1.187 |\n| market_RURAL_IDAHO                               |  0.133 |     1.142 | ***  |      0.010 |   0.000 |        0.113 |             1.120 |        0.153 |             1.165 |\n| market_RURAL_ILLINOIS                            |  0.016 |     1.016 |      |      0.013 |   0.127 |       -0.009 |             0.991 |        0.040 |             1.041 |\n| market_RURAL_INDIANA                             |  0.082 |     1.086 | ***  |      0.010 |   0.000 |        0.062 |             1.064 |        0.103 |             1.108 |\n| market_RURAL_IOWA                                |  0.063 |     1.065 | ***  |      0.018 |   0.000 |        0.028 |             1.029 |        0.097 |             1.102 |\n| market_RURAL_KANSAS                              |  0.127 |     1.136 | ***  |      0.016 |   0.000 |        0.096 |             1.101 |        0.159 |             1.172 |\n| market_RURAL_KENTUCKY                            |  0.149 |     1.160 | ***  |      0.013 |   0.000 |        0.123 |             1.131 |        0.174 |             1.190 |\n| market_RURAL_LOUISIANA                           |  0.047 |     1.048 | ***  |      0.014 |   0.000 |        0.020 |             1.020 |        0.074 |             1.077 |\n| market_RURAL_MAINE                               |  0.079 |     1.082 | ***  |      0.011 |   0.000 |        0.057 |             1.058 |        0.101 |             1.106 |\n| market_RURAL_MICHIGAN                            |  0.074 |     1.076 | ***  |      0.020 |   0.000 |        0.035 |             1.036 |        0.112 |             1.119 |\n| market_RURAL_MINNESOTA                           |  0.178 |     1.195 | ***  |      0.014 |   0.000 |        0.151 |             1.163 |        0.205 |             1.227 |\n| market_RURAL_MISSISSIPPI                         |  0.031 |     1.031 |  **  |      0.012 |   0.025 |        0.008 |             1.008 |        0.053 |             1.055 |\n| market_RURAL_MISSOURI                            |  0.108 |     1.114 | ***  |      0.014 |   0.000 |        0.081 |             1.084 |        0.135 |             1.144 |\n| market_RURAL_MONTANA                             |  0.131 |     1.140 | ***  |      0.022 |   0.000 |        0.088 |             1.092 |        0.173 |             1.189 |\n| market_RURAL_NEBRASKA                            |  0.132 |     1.141 | ***  |      0.012 |   0.000 |        0.108 |             1.115 |        0.155 |             1.168 |\n| market_RURAL_NEVADA                              |  0.046 |     1.048 | ***  |      0.041 |   0.000 |       -0.035 |             0.966 |        0.128 |             1.136 |\n| market_RURAL_NEW_HAMPSHIRE                       |  0.049 |     1.050 |      |      0.013 |   0.238 |        0.024 |             1.024 |        0.074 |             1.077 |\n| market_RURAL_NEW_MEXICO                          |  0.166 |     1.181 | ***  |      0.056 |   0.000 |        0.056 |             1.057 |        0.277 |             1.319 |\n| market_RURAL_NEW_YORK                            | -0.028 |     0.973 |      |      0.011 |   0.626 |       -0.049 |             0.952 |       -0.006 |             0.994 |\n| market_RURAL_NORTH_CAROLINA                      |  0.029 |     1.029 | ***  |      0.020 |   0.008 |       -0.010 |             0.990 |        0.068 |             1.071 |\n| market_RURAL_NORTH_DAKOTA                        |  0.222 |     1.249 | ***  |      0.015 |   0.000 |        0.192 |             1.212 |        0.252 |             1.286 |\n| market_RURAL_OHIO                                |  0.092 |     1.096 | ***  |      0.031 |   0.000 |        0.032 |             1.032 |        0.152 |             1.165 |\n| market_RURAL_OKLAHOMA                            |  0.109 |     1.116 | ***  |      0.033 |   0.000 |        0.044 |             1.045 |        0.175 |             1.191 |\n| market_RURAL_OREGON                              |  0.002 |     1.002 |      |      0.015 |   0.961 |       -0.027 |             0.973 |        0.030 |             1.031 |\n| market_RURAL_PENNSYLVANIA                        |  0.123 |     1.131 | ***  |      0.010 |   0.000 |        0.104 |             1.109 |        0.143 |             1.154 |\n| market_RURAL_SOUTH_CAROLINA                      |  0.055 |     1.057 | ***  |      0.018 |   0.000 |        0.020 |             1.020 |        0.091 |             1.095 |\n| market_RURAL_SOUTH_DAKOTA                        |  0.060 |     1.062 | ***  |      0.013 |   0.001 |        0.034 |             1.035 |        0.086 |             1.090 |\n| market_RURAL_TENNESSEE                           |  0.176 |     1.192 | ***  |      0.010 |   0.000 |        0.157 |             1.170 |        0.195 |             1.215 |\n| market_RURAL_TEXAS                               |  0.168 |     1.183 | ***  |      0.020 |   0.000 |        0.129 |             1.138 |        0.207 |             1.230 |\n| market_RURAL_VERMONT                             |  0.089 |     1.093 | ***  |      0.017 |   0.000 |        0.055 |             1.057 |        0.122 |             1.130 |\n| market_RURAL_VIRGINIA                            |  0.022 |     1.022 |      |      0.014 |   0.191 |       -0.005 |             0.995 |        0.049 |             1.051 |\n| market_RURAL_WASHINGTON                          |  0.109 |     1.116 | ***  |      0.015 |   0.000 |        0.079 |             1.083 |        0.140 |             1.150 |\n| market_RURAL_WEST_VIRGINIA                       | -0.044 |     0.957 | ***  |      0.010 |   0.004 |       -0.064 |             0.938 |       -0.024 |             0.976 |\n| market_RURAL_WISCONSIN                           |  0.037 |     1.037 | ***  |      0.035 |   0.000 |       -0.032 |             0.968 |        0.106 |             1.111 |\n| market_RURAL_WYOMING                             |  0.193 |     1.213 | ***  |      0.011 |   0.000 |        0.172 |             1.188 |        0.214 |             1.239 |\n| market_SACRAMENTO                                |  0.039 |     1.040 | ***  |      0.014 |   0.000 |        0.011 |             1.011 |        0.067 |             1.069 |\n| market_SALT_LAKE_CITY                            |  0.105 |     1.111 | ***  |      0.009 |   0.000 |        0.087 |             1.091 |        0.123 |             1.131 |\n| market_SAN_ANTONIO                               |  0.135 |     1.144 | ***  |      0.012 |   0.000 |        0.112 |             1.119 |        0.158 |             1.171 |\n| market_SAN_DIEGO                                 |  0.013 |     1.013 |      |      0.011 |   0.259 |       -0.008 |             0.992 |        0.034 |             1.035 |\n| market_SAN_FRANCISCO                             |  0.069 |     1.072 | ***  |      0.011 |   0.000 |        0.048 |             1.049 |        0.091 |             1.095 |\n| market_SEATTLE                                   |  0.118 |     1.125 | ***  |      0.010 |   0.000 |        0.098 |             1.103 |        0.137 |             1.147 |\n| market_ST_LOUIS                                  |  0.045 |     1.046 | ***  |      0.013 |   0.000 |        0.019 |             1.019 |        0.071 |             1.073 |\n| market_SURBURBAN_NJ                              | -0.034 |     0.967 |  **  |      0.012 |   0.011 |       -0.058 |             0.943 |       -0.010 |             0.991 |\n| market_SURBURBAN_NY                              |  0.118 |     1.126 | ***  |      0.015 |   0.000 |        0.089 |             1.094 |        0.147 |             1.159 |\n| market_SYRACUSE                                  | -0.064 |     0.938 | ***  |      0.009 |   0.000 |       -0.082 |             0.921 |       -0.047 |             0.955 |\n| market_TAMPA                                     |  0.113 |     1.120 | ***  |      0.011 |   0.000 |        0.091 |             1.095 |        0.135 |             1.144 |\n| market_URBAN_NY                                  |  0.162 |     1.176 | ***  |      0.011 |   0.000 |        0.141 |             1.151 |        0.184 |             1.202 |\n| market_WASHINGTON_DC                             |  0.092 |     1.097 | ***  |      0.060 |   0.000 |       -0.025 |             0.975 |        0.210 |             1.234 |\n| brand_BUSCH_LIGHT_*_promo_True                   | -0.238 |     0.788 | ***  |      0.045 |   0.000 |       -0.326 |             0.722 |       -0.151 |             0.860 |\n| brand_COORS_LIGHT_*_promo_True                   | -0.153 |     0.859 | ***  |      0.036 |   0.001 |       -0.224 |             0.800 |       -0.081 |             0.922 |\n| brand_MILLER_LITE_*_promo_True                   | -0.260 |     0.771 | ***  |      0.034 |   0.000 |       -0.328 |             0.720 |       -0.193 |             0.825 |\n| brand_NATURAL_LIGHT_*_promo_True                 | -0.412 |     0.663 | ***  |      0.004 |   0.000 |       -0.420 |             0.657 |       -0.403 |             0.668 |\n| brand_BUSCH_LIGHT_*_log_beer_floz                | -0.015 |           | ***  |      0.004 |   0.000 |       -0.022 |                   |       -0.008 |                   |\n| brand_COORS_LIGHT_*_log_beer_floz                | -0.007 |           |  *   |      0.003 |   0.069 |       -0.013 |                   |       -0.000 |                   |\n| brand_MILLER_LITE_*_log_beer_floz                | -0.019 |           | ***  |      0.003 |   0.000 |       -0.026 |                   |       -0.013 |                   |\n| brand_NATURAL_LIGHT_*_log_beer_floz              |  0.041 |           | ***  |      0.001 |   0.000 |        0.040 |                   |        0.042 |                   |\n| promo_True_*_log_beer_floz                       | -0.008 |           | ***  |      0.011 |   0.000 |       -0.029 |                   |        0.013 |                   |\n| brand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz   |  0.045 |           | ***  |      0.008 |   0.000 |        0.030 |                   |        0.061 |                   |\n| brand_COORS_LIGHT_*_promo_True_*_log_beer_floz   |  0.024 |           | ***  |      0.007 |   0.003 |        0.011 |                   |        0.037 |                   |\n| brand_MILLER_LITE_*_promo_True_*_log_beer_floz   |  0.047 |           | ***  |      0.006 |   0.000 |        0.035 |                   |        0.059 |                   |\n| brand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz |  0.072 |           | ***  |      0.014 |   0.000 |        0.045 |                   |        0.100 |                   |\n| Intercept                                        | -2.096 |           | ***  |      0.002 |         |       -2.100 |                   |       -2.092 |                   |\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| Observations                                     | 48,153 |           |      |            |         |              |                   |              |                   |\n| R²                                               |  0.555 |           |      |            |         |              |                   |              |                   |\n| RMSE                                             |  0.167 |           |      |            |         |              |                   |              |                   |\n+--------------------------------------------------+--------+-----------+------+------------+---------+--------------+-------------------+--------------+-------------------+\n\n\n\n# For log_beer_floz, There is a negative relationship between the volume of beer and the price per fluid ounce,\n# meaning that as the beer volume increases, the price per fluid ounce decreases. Larger beer brands (like MILLER_LITE)\n# tend to increase the price per fluid ounce, while budget-friendly brands like BUSCH_LIGHT and NATURAL_LIGHT result in lower prices.\n\n\n\nQ4\n\n\n\nimage.png\n\n\n\nprint(\n    compare_reg_models(\n        [model_1, model_2, model_3],\n        [assembler_1, assembler_2, assembler_3]\n        )\n    )\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| Predictor                                        |   y: log_price_per_floz (Model 1) |   y: log_price_per_floz (Model 2) |   y: log_price_per_floz (Model 3) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n+==================================================+===================================+===================================+===================================+\n| log_beer_floz                                    |                         -0.149*** |                         -0.155*** |                         -0.151*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_BUSCH_LIGHT                                |                 -0.274*** / 0.760 |                 -0.233*** / 0.792 |                 -0.196*** / 0.822 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_COORS_LIGHT                                |                  -0.006** / 0.994 |                     0.021 / 1.021 |                    0.033* / 1.033 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_MILLER_LITE                                |                 -0.014*** / 0.986 |                  0.064*** / 1.066 |                  0.091*** / 1.095 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_NATURAL_LIGHT                              |                 -0.334*** / 0.716 |                 -0.633*** / 0.531 |                 -0.554*** / 0.575 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_ALBANY                                    |                   0.031** / 1.031 |                  0.033*** / 1.033 |                   0.025** / 1.025 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_ATLANTA                                   |                  0.088*** / 1.092 |                  0.086*** / 1.090 |                  0.083*** / 1.087 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_BALTIMORE                                 |                  0.095*** / 1.100 |                  0.099*** / 1.104 |                  0.090*** / 1.094 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_BIRMINGHAM                                |                  0.126*** / 1.135 |                  0.133*** / 1.142 |                  0.128*** / 1.137 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_BOSTON                                    |                  0.124*** / 1.132 |                  0.124*** / 1.132 |                  0.121*** / 1.129 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_CHARLOTTE                                 |                    0.017* / 1.017 |                     0.013 / 1.013 |                   0.023** / 1.023 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_CHICAGO                                   |                    -0.000 / 1.000 |                    -0.006 / 0.994 |                     0.001 / 1.001 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_CINCINNATI                                |                  0.087*** / 1.091 |                  0.082*** / 1.085 |                  0.080*** / 1.084 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_CLEVELAND                                 |                  0.066*** / 1.069 |                  0.061*** / 1.063 |                  0.059*** / 1.061 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_COLUMBUS                                  |                  0.079*** / 1.082 |                  0.075*** / 1.078 |                  0.075*** / 1.078 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_DALLAS                                    |                  0.210*** / 1.233 |                  0.219*** / 1.245 |                  0.224*** / 1.252 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_DENVER                                    |                  0.127*** / 1.135 |                  0.124*** / 1.132 |                  0.135*** / 1.145 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_DES_MOINES                                |                  0.135*** / 1.145 |                  0.132*** / 1.141 |                  0.126*** / 1.134 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_DETROIT                                   |                  0.094*** / 1.098 |                  0.089*** / 1.093 |                  0.092*** / 1.096 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_EXURBAN_NJ                                |                  0.253*** / 1.288 |                  0.246*** / 1.279 |                  0.238*** / 1.269 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_EXURBAN_NY                                |                  0.124*** / 1.132 |                  0.120*** / 1.127 |                  0.116*** / 1.123 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_GRAND_RAPIDS                              |                  0.083*** / 1.086 |                  0.079*** / 1.082 |                  0.078*** / 1.081 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_HARTFORD-NEW_HAVEN                        |                  0.137*** / 1.147 |                  0.134*** / 1.144 |                  0.133*** / 1.143 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_HOUSTON                                   |                  0.122*** / 1.130 |                  0.119*** / 1.126 |                  0.123*** / 1.130 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_INDIANAPOLIS                              |                  0.056*** / 1.058 |                  0.055*** / 1.056 |                  0.057*** / 1.059 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_JACKSONVILLE                              |                  0.137*** / 1.147 |                  0.130*** / 1.139 |                  0.132*** / 1.141 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_KANSAS_CITY                               |                  0.084*** / 1.087 |                  0.078*** / 1.081 |                  0.074*** / 1.077 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_LITTLE_ROCK                               |                  0.097*** / 1.102 |                  0.093*** / 1.097 |                  0.090*** / 1.094 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_LOS_ANGELES                               |                  0.029*** / 1.030 |                   0.023** / 1.023 |                  0.032*** / 1.033 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_LOUISVILLE                                |                  0.068*** / 1.070 |                  0.063*** / 1.065 |                  0.068*** / 1.070 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_MEMPHIS                                   |                  0.135*** / 1.145 |                  0.134*** / 1.144 |                  0.128*** / 1.137 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_MIAMI                                     |                  0.120*** / 1.128 |                  0.119*** / 1.126 |                  0.120*** / 1.128 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_MILWAUKEE                                 |                   0.030** / 1.030 |                   0.029** / 1.029 |                  0.031*** / 1.031 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_MINNEAPOLIS                               |                  0.131*** / 1.140 |                  0.132*** / 1.141 |                  0.129*** / 1.138 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_NASHVILLE                                 |                  0.156*** / 1.168 |                  0.155*** / 1.168 |                  0.154*** / 1.166 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_NEW_ORLEANS-MOBILE                        |                  0.136*** / 1.146 |                  0.126*** / 1.135 |                  0.122*** / 1.129 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_OKLAHOMA_CITY-TULSA                       |                  0.144*** / 1.154 |                  0.140*** / 1.151 |                  0.134*** / 1.144 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_OMAHA                                     |                  0.120*** / 1.128 |                  0.117*** / 1.125 |                  0.120*** / 1.128 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_ORLANDO                                   |                  0.110*** / 1.117 |                  0.108*** / 1.114 |                  0.112*** / 1.118 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_PHILADELPHIA                              |                  0.105*** / 1.111 |                  0.105*** / 1.110 |                  0.094*** / 1.099 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_PHOENIX                                   |                  0.147*** / 1.159 |                  0.150*** / 1.162 |                  0.159*** / 1.172 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_PITTSBURGH                                |                  0.092*** / 1.096 |                  0.090*** / 1.094 |                  0.085*** / 1.089 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_PORTLAND                                  |                  0.121*** / 1.129 |                  0.119*** / 1.126 |                  0.122*** / 1.129 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RALEIGH-DURHAM                            |                  0.097*** / 1.102 |                  0.096*** / 1.101 |                  0.092*** / 1.097 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RICHMOND                                  |                  0.038*** / 1.039 |                  0.035*** / 1.036 |                  0.030*** / 1.031 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_ALABAMA                             |                  0.162*** / 1.176 |                  0.162*** / 1.175 |                  0.158*** / 1.171 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_ARKANSAS                            |                  0.181*** / 1.198 |                  0.181*** / 1.199 |                  0.175*** / 1.191 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_CALIFORNIA                          |                  0.042*** / 1.043 |                  0.038*** / 1.039 |                  0.042*** / 1.043 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_COLORADO                            |                  0.167*** / 1.181 |                  0.164*** / 1.178 |                  0.171*** / 1.187 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_FLORIDA                             |                  0.072*** / 1.074 |                  0.063*** / 1.065 |                  0.062*** / 1.064 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_GEORGIA                             |                  0.142*** / 1.153 |                  0.140*** / 1.150 |                  0.135*** / 1.145 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_IDAHO                               |                  0.141*** / 1.152 |                  0.133*** / 1.142 |                  0.133*** / 1.142 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_ILLINOIS                            |                    0.019* / 1.019 |                    0.017* / 1.017 |                     0.016 / 1.016 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_INDIANA                             |                  0.079*** / 1.082 |                  0.080*** / 1.083 |                  0.082*** / 1.086 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_IOWA                                |                  0.069*** / 1.072 |                  0.065*** / 1.067 |                  0.063*** / 1.065 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_KANSAS                              |                  0.134*** / 1.143 |                  0.134*** / 1.143 |                  0.127*** / 1.136 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_KENTUCKY                            |                  0.152*** / 1.165 |                  0.151*** / 1.163 |                  0.149*** / 1.160 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_LOUISIANA                           |                  0.060*** / 1.062 |                  0.056*** / 1.057 |                  0.047*** / 1.048 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MAINE                               |                  0.083*** / 1.087 |                  0.079*** / 1.083 |                  0.079*** / 1.082 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MICHIGAN                            |                  0.081*** / 1.085 |                  0.078*** / 1.081 |                  0.074*** / 1.076 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MINNESOTA                           |                  0.180*** / 1.197 |                  0.183*** / 1.201 |                  0.178*** / 1.195 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MISSISSIPPI                         |                  0.037*** / 1.037 |                   0.031** / 1.032 |                   0.031** / 1.031 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MISSOURI                            |                  0.114*** / 1.121 |                  0.114*** / 1.120 |                  0.108*** / 1.114 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_MONTANA                             |                  0.127*** / 1.135 |                  0.124*** / 1.132 |                  0.131*** / 1.140 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NEBRASKA                            |                  0.137*** / 1.147 |                  0.136*** / 1.146 |                  0.132*** / 1.141 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NEVADA                              |                  0.049*** / 1.050 |                  0.048*** / 1.049 |                  0.046*** / 1.048 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NEW_HAMPSHIRE                       |                     0.068 / 1.070 |                     0.053 / 1.055 |                     0.049 / 1.050 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NEW_MEXICO                          |                  0.173*** / 1.189 |                  0.168*** / 1.183 |                  0.166*** / 1.181 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NEW_YORK                            |                    -0.018 / 0.982 |                    -0.015 / 0.985 |                    -0.028 / 0.973 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NORTH_CAROLINA                      |                     0.011 / 1.011 |                  0.040*** / 1.041 |                  0.029*** / 1.029 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_NORTH_DAKOTA                        |                  0.223*** / 1.249 |                  0.222*** / 1.248 |                  0.222*** / 1.249 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_OHIO                                |                  0.097*** / 1.102 |                  0.095*** / 1.099 |                  0.092*** / 1.096 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_OKLAHOMA                            |                  0.119*** / 1.126 |                  0.119*** / 1.126 |                  0.109*** / 1.116 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_OREGON                              |                     0.005 / 1.005 |                    -0.000 / 1.000 |                     0.002 / 1.002 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_PENNSYLVANIA                        |                  0.132*** / 1.141 |                  0.133*** / 1.142 |                  0.123*** / 1.131 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_SOUTH_CAROLINA                      |                  0.055*** / 1.056 |                  0.053*** / 1.054 |                  0.055*** / 1.057 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_SOUTH_DAKOTA                        |                  0.065*** / 1.067 |                  0.062*** / 1.064 |                  0.060*** / 1.062 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_TENNESSEE                           |                  0.173*** / 1.189 |                  0.173*** / 1.189 |                  0.176*** / 1.192 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_TEXAS                               |                  0.173*** / 1.188 |                  0.171*** / 1.186 |                  0.168*** / 1.183 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_VERMONT                             |                  0.101*** / 1.107 |                  0.089*** / 1.093 |                  0.089*** / 1.093 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_VIRGINIA                            |                     0.025 / 1.025 |                     0.023 / 1.024 |                     0.022 / 1.022 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_WASHINGTON                          |                  0.096*** / 1.101 |                  0.094*** / 1.099 |                  0.109*** / 1.116 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_WEST_VIRGINIA                       |                  -0.032** / 0.968 |                  -0.037** / 0.964 |                 -0.044*** / 0.957 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_WISCONSIN                           |                  0.040*** / 1.041 |                  0.038*** / 1.038 |                  0.037*** / 1.037 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_RURAL_WYOMING                             |                  0.198*** / 1.219 |                  0.197*** / 1.218 |                  0.193*** / 1.213 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SACRAMENTO                                |                  0.032*** / 1.032 |                  0.030*** / 1.031 |                  0.039*** / 1.040 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SALT_LAKE_CITY                            |                  0.113*** / 1.119 |                  0.107*** / 1.112 |                  0.105*** / 1.111 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SAN_ANTONIO                               |                  0.143*** / 1.154 |                  0.138*** / 1.148 |                  0.135*** / 1.144 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SAN_DIEGO                                 |                     0.009 / 1.009 |                     0.008 / 1.008 |                     0.013 / 1.013 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SAN_FRANCISCO                             |                  0.065*** / 1.067 |                  0.062*** / 1.064 |                  0.069*** / 1.072 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SEATTLE                                   |                  0.114*** / 1.121 |                  0.104*** / 1.110 |                  0.118*** / 1.125 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_ST_LOUIS                                  |                  0.046*** / 1.047 |                  0.042*** / 1.043 |                  0.045*** / 1.046 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SURBURBAN_NJ                              |                    -0.022 / 0.979 |                   -0.024* / 0.976 |                  -0.034** / 0.967 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SURBURBAN_NY                              |                  0.122*** / 1.130 |                  0.118*** / 1.125 |                  0.118*** / 1.126 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_SYRACUSE                                  |                 -0.051*** / 0.950 |                 -0.056*** / 0.945 |                 -0.064*** / 0.938 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_TAMPA                                     |                  0.115*** / 1.122 |                  0.111*** / 1.117 |                  0.113*** / 1.120 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_URBAN_NY                                  |                  0.163*** / 1.177 |                  0.162*** / 1.176 |                  0.162*** / 1.176 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| market_WASHINGTON_DC                             |                  0.104*** / 1.110 |                  0.097*** / 1.102 |                  0.092*** / 1.097 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_BUSCH_LIGHT_*_log_beer_floz                |                                   |                           -0.007* |                         -0.015*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_COORS_LIGHT_*_log_beer_floz                |                                   |                            -0.005 |                           -0.007* |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_MILLER_LITE_*_log_beer_floz                |                                   |                         -0.015*** |                         -0.019*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_NATURAL_LIGHT_*_log_beer_floz              |                                   |                          0.056*** |                          0.041*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_BUSCH_LIGHT_*_promo_True                   |                                   |                                   |                 -0.238*** / 0.788 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_COORS_LIGHT_*_promo_True                   |                                   |                                   |                 -0.153*** / 0.859 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_MILLER_LITE_*_promo_True                   |                                   |                                   |                 -0.260*** / 0.771 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_NATURAL_LIGHT_*_promo_True                 |                                   |                                   |                 -0.412*** / 0.663 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| promo_True_*_log_beer_floz                       |                                   |                                   |                         -0.008*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz   |                                   |                                   |                          0.045*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_COORS_LIGHT_*_promo_True_*_log_beer_floz   |                                   |                                   |                          0.024*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_MILLER_LITE_*_promo_True_*_log_beer_floz   |                                   |                                   |                          0.047*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| brand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz |                                   |                                   |                          0.072*** |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| Intercept                                        |                         -2.115*** |                         -2.079*** |                         -2.096*** |\n================================================================================================================================================================\n| Observations                                     |                            48,153 |                            48,153 |                            48,153 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| R²                                               |                             0.543 |                             0.548 |                             0.555 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n| RMSE                                             |                             0.170 |                             0.169 |                             0.167 |\n+--------------------------------------------------+-----------------------------------+-----------------------------------+-----------------------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n# For model one, the beta estimate log_beer_floz is 0.149\n\n\n# For model two, we want to subtract the log_beer_floz beta (-0.155) on brand_BUSCH_LIGHT_*_log_beer_floz (-0.007),\n# brand_MILLER_LITE_*_log_beer_floz (-0.015), brand_NATURAL_LIGHT_*_log_beer_floz (0.05) and we skip\n# brand_COORS_LIGHT_*_log_beer_floz (-0.005) since it is not significant. So that means 0.155 subtracted with the respective brand&log\n# are -0.148 (brand_BUSCH_LIGHT_*_log_beer_floz), -0.14 (brand_MILLER_LITE_*_log_beer_floz), -0.205 (brand_NATURAL_LIGHT_*_log_beer_floz)\n\n\n# For model three, we want to do the same with log_beer_floz beta (-0.151) any brand*promo*log so brand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz(0.045),\n# brand_COORS_LIGHT_*_promo_True_*_log_beer_floz(0.024), brand_MILLER_LITE_*_promo_True_*_log_beer_floz (0.047), brand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz (0.072).\n# If we subtract -0.151 (log_beer_floz beta) with their respective betas which are -0.196 (brand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz), -0.175 (brand_COORS_LIGHT_*_promo_True_*_log_beer_floz),\n# -0.198 (brand_MILLER_LITE_*_promo_True_*_log_beer_floz), and -0.223 (brand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz)\n\n\n\nQ5\n\n\n\nimage.png\n\n\n\n# market_ALBANY has a beta 0.025 which suggest Albany market is 2.5% higher than the reference market.\n# market_EXURBAN_NY has a beta 0.116 which means 11.6% higher than the reference market.\n# market_RURAL_NEW_YORK has a beta 0.028 and is not significant\n# market_SURBURBAN_NY has a beta 0.118 where it is 11.8% higher than the reference market.\n# market_SYRACUSE has a beta -0.064 where it is 6.4% lower than the reference market\n# market_URBAN_NY has a beta 0.162 where it is 16.2% higher than the reference market.\n\n\n\nQ6\n\n\n\nimage.png\n\n\n\n# The coefficient for log_beer_floz across all three models is negative and statistically significant\n# Model 1: -0.149\n# Model 2: -0.155\n# Model 3: -0.151\n# A 1% increase in the volume of beer purchased reduces the price per ounce by approximately 0.149%(model 1) to 0.155%(model 2) to 0.151% (model 3)\n\n# The interaction terms between promo and log_beer_floz for each brand are positive and significant.\n# This suggests that when a promotion is active, the price sensitivity to changes in volume (beer purchased) becomes more positive.\n\n\n\nQ7\n\n\n\nimage.png\n\n\n\nresidual_plot(dtest_1, \"log_price_per_floz\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_2, \"log_price_per_floz\", \"Model 2\")\n\n\n\n\n\n\n\n\n\nresidual_plot(dtest_3, \"log_price_per_floz\", \"Model 3\")\n\n\n\n\n\n\n\n\n\n\nQ8\n\n# Create a new column for squared error\ndtest_1 = dtest_1.withColumn(\"error_sq\", pow(col(\"log_price_per_floz\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_1.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.171\n\n\n\n# Create a new column for squared error\ndtest_2 = dtest_2.withColumn(\"error_sq\", pow(col(\"log_price_per_floz\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_2.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.170\n\n\n\n# Create a new column for squared error\ndtest_3 = dtest_3.withColumn(\"error_sq\", pow(col(\"log_price_per_floz\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_3.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.169\n\n\n\n# Model three is model I prefer because on the model the dotted line is closest\n# to the blue line and the RMSE is lower on Model 3 then Model 1 and 2 ensurining\n# Model 3 to be the most accurate out of the models."
  },
  {
    "objectID": "posts/Final Project Paper/DANL_320_Final_Paper.html",
    "href": "posts/Final Project Paper/DANL_320_Final_Paper.html",
    "title": "Predicting Bank Term Deposit Success with Machine Learning",
    "section": "",
    "text": "Predicting Bank Term Deposit Success with Machine Learning\nDaniel Xu\nDANL 320: Big Data Analytics\nDr. Byeong-Hak Choe\nMay 13, 2025\n\n\nIntroduction\nIn the competitive landscape of the financial sector, banks constantly seek new methods to gather potential new clients and generate stable streams of revenue for profit. Among various banking products, term deposits serve as a cornerstone of income for financial institutions. Term deposits, also known as a fixed deposit or time deposit, is a financial product where you agree to deposit a specific amount of money for a set period and earn a fixed interest rate. These fixed-term investments offer customers a secure way to earn interest over a predetermined period which ensures higher interest rates than traditional liquid savings accounts. For banks, this is essential as it allows banks to invest the money in other financial products that pay a higher rate of return (RoR) than what the bank is paying the customer for the use of their funds whilst maintaining liquidity and stabilizing funding within their company. Additionally, clients who do opt in for a term deposit can not withdraw the money or face a penalty depending on which banking institution or term deposit.\nTo promote term deposits effectively, many banks employ a wide array of marketing strategies, including digital advertisements, email marketing, social media outreach, and phone calls to potential clients. With the evolution of technology, telephone-based marketing remains one of the most effective ways to get new clients especially for banks. It allows the representative of the bank to have a direct and personal conversation with the clients and allows for real-time customer engagement, clarifications, and persuasion. However, this comes with a cost particularly in regards with the amount of time and calls the representative has to do for one potential client resulting in cost-inefficiency. By using data-driven insights on potential clients who are more likely to subscribe to a term deposit will allow the banks to improve efficiency, reduce costs, and maximize returns on marketing investments.\nThis project will use a dataset from Kaggle where all the data is derived from the direct marketing efforts of a Portuguese banking institution that primarily focused on telephone-based marketing. The dataset includes over 45,000 client records, each with detailed demographic, socio-economic, and campaign-related information which will be used to determine the potential clients to reach out to for banks. In this dataset, the target variable is ‘y’ for term deposits which have binary outcomes of “yes” or “no”. The key variables selected for analysis in this study will include age (age of the client), job type (the client’s occupation), marital status, education level, and default status (whether the client has credit in default). In addition, financial obligations such as having a housing loan or personal loan and campaign-specific variables include the duration of the last contact (in seconds) and the number of contacts performed during the campaign will be included into the regression. For this project, the two main objectives are to find out which factors are the most important in determining whether a client will subscribe to a term deposit (y) which will allow banks to prioritize clients who have those specific variables. The second main objective will be to determine what actions banks can take to increase the likelihood of clients subscribing to term deposits based on the key findings. By identifying which characteristics are most closely associated with the positive outcomes, banks will be more informed, have data-driven marketing strategies that improve targeting, reduced operational costs, and ultimately boost customer conversion rates.\n\n\nData\nThe data used in this project originates from Kaggle, where it is based on the direct telemarketing campaigns conducted by a Portuguese banking institution. The data is publicly available and originally sourced from the UCI Machine Learning Repository, as part of a research effort detailed in the studies by Moro, Cortez, and Rita (2014). The data was “collected from 2008 to 2013” (Moro) and contains 45,211 observations in the training set and 4,521 in the test set, representing client-level data. The dataset is structured into 18 columns, which include both categorical and numerical variables where ‘y’ or term deposit is the target variable. As previously mentioned, for this project the key variables selected for analysis includes age, job type, marital status, education level, default status, housing loan, personal loan, duration of the last contact, and number of contacts made during the campaign. The demographic variables which describe the personal and social characteristics of the client are age, job type, marital status, and education. The financial variables that relate to the client’s financial situation are default status, housing loan, and personal loan. Additionally, the numeric variables used in this analysis include age, duration of the last contact, and the number of contacts made during the campaign while, the categorical variables include job type, marital status, education level, default status, housing loan, and personal loan.\nAfter loading the dataset into Google Colab, I had to make some changes to clean the data before running any models. Some values in the categorical variables, like basic.4y in the education column, contained periods which caused errors during the regression process. To fix that issue I changed all the periods in underscores which the machine learning models can recognize as a space and won’t interfere with the regressions.\nFigure 1. Value renaming for variables in the dataframe\n\n\n\nimage_2025-05-14_185039739.png\n\n\nAs seen in Figure 1, the same code was applied to job, marital status, default, housing, loan, and contact since the column variables all contained values with periods. Additionally, the variable ‘y’ or term deposit had categorical values of ‘yes’ or ‘no’ which needed to be changed to 1 for ‘yes’ and 0 for ‘no’ allowing the regressions to run smoothly. For the project, I removed the ‘unknown’ from ‘loan’ as it caused perfect multicollinearity and caused errors in the machine learning models. Finally, I created dummy variables for all the categorical columns such as job, marital status, education, default status, housing loan, and personal loan as seen in Figure 2 for the machine learning models.\nFigure 2. Creation of dummy variables\n\n\n\nimage.png\n\n\nIn Figure 3, a summary of the key numerical variables like age, duration, and campaign which helps provide insight into the client base and the nature of the bank’s marketing outreach. The average client age was about 40 years old, suggesting that most individuals targeted in the campaign are middle-aged. The duration variable, which represents the length of the last phone call, has an average of around 258 seconds, meaning most calls lasted just a few minutes. The campaign variable, showing how many times a client was contacted, averages about 2.5 calls per person. Figure 4 shows a histogram of the variable duration, which represents the length of the last phone call in seconds. The chart reveals that around 25,000 clients ended their calls within the first 2.5 minutes. This suggests that a large portion of the clients may not have been interested in the offer, as many seemed to end the call quickly. With this in mind, it appears that maintaining client attention is a challenge in these marketing campaigns. Overall, the histogram provides useful insight into client behavior and highlights the importance of making a strong impression early in the call.\nFigure 3. Descriptive statistics of age, duration, and campaign\n\n\n\nimage.png\n\n\nFigure 4. Histogram of Duration\n\n\n\nimage.png\n\n\nLastly, the final bar plot shows the distribution of the target variable (y or term deposit), with the response as ‘0’ for “no” and ‘1’ for “yes.” Additionally looking at the bar plot, the majority of the clients around 35,000 did not subscribe to a term deposit, while only about 5,000 subscribed. This indicates that there is a significant class imbalance in the data, with the “no” outcomes greatly out numbering the “yes” responses. This can lead to issues with model performance because many machine learning models will favor the majority class when making predictions.\nFigure 5.Bar Plot of y\n\n\n\nimage.png\n\n\nIn Figure 6, I created a heat correlation map using the numerical variables in the dataset. This visual tool helps identify multicollinearity, which occurs when two or more variables are highly correlated (typically above 0.8 or below –0.8). Multicollinearity can be problematic because it can distort the results of regression models by inflating the variance of coefficient estimates. One of the most notable observations from the heatmap is the strong positive correlation between the variable duration and the target variable y (term deposit), with a correlation value of approximately 0.8 as shown in Figure 6. This suggests that the longer a phone call lasts, the more likely it is that the client will agree to subscribe to a term deposit. This makes sense intuitively, as longer calls may indicate higher engagement or interest from the client. By highlighting these relationships, the heatmap provides useful insights that can inform which variables to prioritize or monitor closely when fitting regression models.\nFigure 6. Heat Correlation Map of variables and y\n\n\n\nimage.png\n\n\n\n\nMachine Learning Model\nFor this project, I employed several machine learning models, including linear regression, logistic regression, dual density plot, pruned decision tree, random forest, and gradient boosting. Since the target variable, ‘y’, is binary, where 1 represents a client subscribing to a term deposit and 0 means they did not, the primary task is classification. Logistic regression is particularly well-suited for this analysis because it models the probability of a binary outcome and enables straightforward interpretation of the influence of individual variables. Linear regression was also included as a baseline model, although it is not ideal for binary outcomes, it helps highlight the improvements gained through classification techniques. Additionally, tools such as the confusion matrix and dual density plots were used to evaluate model performance and visualize the separation between classes. The decision tree-based models, like random forest and gradient boosting, were used to improve predictive accuracy and assess variable importance.\nThe linear regression model, as shown in Figure 7, includes 28,137 observations and produces an R² value of 0.197. This means that approximately 19.7% of the variance in the target variable (y)—whether a client subscribes to a term deposit—is explained by the model. The Root Mean Square Error (RMSE) is 0.283, indicating a moderate level of prediction error. One key insight is that duration has a strong positive relationship with y, supporting earlier observations that longer phone calls are associated with a higher chance of success. Certain job types, such as student (coefficient of 0.142) and retired (0.107), are positively associated with subscription, suggesting these groups are more responsive to the campaign. On the other hand, jobs like blue-collar and entrepreneur have negative coefficients, indicating a lower likelihood of agreeing to a term deposit offer.\nFigure 7. Linear regression\n\n\n\nimage.png\n\n\nThe logistic regression model in Figure 8 reveals several important insights about the likelihood of a client subscribing to a term deposit. The residual deviance of 15,256.47 and an AIC of 15,314.47 indicate a decent model fit compared to the null deviance of 19,753.24. The variable duration has the most substantial positive effect on the outcome with a coefficient 0.0040, confirming that longer calls significantly increase the odds of a ‘yes’. Similarly, client age has a modest positive effect where β = 0.0151, while the number of contacts during the campaign is negatively associated with success where β = -0.1296, suggesting that repeated calls may lower effectiveness. In terms of job roles, students had a β of 1.0812 and retired individuals had a β of 0.8392 are much more likely to say ‘yes’ while blue-collar had a β of -0.5617 and entrepreneur had a β of -0.5592 where the roles show negative associations with success. The education level university degree also has a significant positive effect β of 0.2100, while basic 9-year education is negatively associated with subscription. Overall, the logistic regression model offers stronger interpretability than linear regression for this binary classification task and confirms that call duration and client profile significantly affect campaign outcomes.\nFigure 8. Logistic Regression\n\n\n\nimage.png\n\n\nIn addition, a dual density plot (Figure 9) was employed to show the logistic regression model’s predicted probabilities for the positive and negative classes. The cut-off of 0.11 selected here is significantly less than the traditional 0.5 cut-off, so the model becomes more liberal in marking an observation as a positive case. This reduced threshold has the purpose of enhancing recall by capturing more true positives, something that is especially useful in situations where failure to capture a positive case comes at a high cost. The greater recall is achieved at the expense of precision, however, as it captures more false positives. The narrative readily illustrates the overlap of distribution at this loose threshold, emphasizing the trade-off between recall and precision and highlighting the need to set the threshold to the exact objectives.\nFigure 9. Dual Density Plot\n\n\n\nimage.png\n\n\nThe pruned regression tree as seen in Figure 10 for predicting y demonstrates a model with a training mean squared error (MSE) of 0.078 and a test MSE of 0.079, indicating strong generalization and minimal overfitting. The tree uses key features such as duration, age, and default status to split the data, with duration being the most influential predictor at the root node. We see lower values of duration and age tend to correspond to lower predicted probabilities, while longer durations and certain profiles, like non-students with unknown default status, can lead to higher predictions of the target variable. Leaf nodes with higher predicted values often have increased squared error and fewer samples, reflecting more uncertain predictions. Overall, the pruned tree balances complexity and interpretability, giving us valuable insight while maintaining predictive accuracy.\nFigure 10. Prunned Tree\n\n\n\nimage.png\n\n\nThe random forest model as shown in Figure 11 was built using 500 trees and allowed up to 5 features per split where it performed very well on the training data, with a low mean squared error (MSE) of 0.0112. However, the model’s performance was weaker as it was defined by the out-of-bag (OOB) score which was around 0.179 and the test MSE was higher at 0.0825, suggesting that the model may not generalize well beyond the training set. This can also be seen in the observed vs. predicted graph, where the predicted values are mostly clustered near 0 and 1 where it is matching the binary nature of the target and many do not line up closely with the actual values. Ideally, points would fall along the red dashed diagonal line (indicating perfect predictions), but the noticeable vertical spread shows that the model frequently misses the true probability. This may be due to imbalanced data or limitations in the features used, and it suggests that the model captures general patterns but lacks accuracy in estimating probabilities more precisely.\nFigure 11. Random Forest graph\n\n\n\nimage.png\n\n\nThe importance of the variables as seen in Figure 12 from the random forest regression model show the importance of various features in deciding whether a customer will subscribe to a term deposit. The highest ranked feature is duration (0.4865), where it was the strongest feature, showing that the duration of the last phone call is most significant in deciding probability of subscription. Age (0.2286) also has high importance, where it states older clients would most likely subscribe. Campaign (0.0716) is of medium importance, and this is the interactions with the client during the course of the campaign as a cause factor. On the other hand, job type attributes have very low in significance, with some examples like job_services (0.0059) and job_technician (0.0091) contributing to a low amount to the model predictions, suggesting that job type is not a significant influence in whether or not a client will take out a term deposit. Similarly, marital status and education level also carry a moderate importance with characteristics like marital_single (0.0143) and education_high_school (0.0113), where there is some effect, although not anywhere close to that of the age and tenure cases. The characteristics loan (0.0200) and housing (0.0289) show that taking a personal loan or a housing loan also has a limited effect on subscription likelihood. In general, this analysis identifies the variables that exert the greatest control over client probability to subscribe to a term deposit that can be utilized to prevent wasteful and inefficient marketing tactics.)\nFigure 11. Variable Importance\n\n\n\nimage.png\n\n\nThe Gradient Boosting model in Figure 13 shows a Test MSE of 0.08096, indicating a moderate error, suggesting the model performs reasonably well in predicting term deposit subscriptions. The variable importance analysis reveals that duration holds the highest importance, as it is frequently used in splits across all trees, underlining its critical role in predicting whether a client will subscribe to a term deposit. Age follows as the second most important feature, reinforcing its significance as a key indicator in the prediction process. These insights highlight that both duration and age are essential features in the model, with duration being particularly influential.\nFigure 13. Gradient Boosting\n\n\n\nimage.png\n\n\n\n\nPolicy Applications\nIn this project, several variables were determined to directly influence the likelihood of a client enrolling in a term deposit. Call duration was among the most significant factors determined, and it revealed that longer and more meaningful calls with clients significantly increase the likelihood of securing a term deposit. This means that meaningful client engagement in calls significantly increases the likelihood of a successful client for a term deposit. Age was the other important factor in which older clients, especially those who were 50 years and above, would most likely purchase a term deposit so that the bank should focus on clients who would know the safety of the fixed interest rate. On the other hand, factors like job class and education level were the least predictive in more than one model, such as random forest and gradient boosting. This implies that the client’s profession or educational status does not have a great impact on whether they will subscribe to a term deposit. Therefore, depending on these characteristics to inform marketing segmentation may result in wastage of resources and possibly miss more profitable prospects. There are a couple of other variables in the middle importance range and, while less predictive on their own, could still provide useful information if combined with more powerful predictors. Some variables include marital status, housing loan, and personal loan, each of which may not do much to change the model’s prediction by itself but that can improve performance when used with more predictive features such as call duration and age.\nBased on these findings, banks can adopt a variety of measures to increase their term deposit subscription rates. Bank employees should be trained to focus on quality rather than quantity in client interactions, prioritizing explanation, listening, and trust-building throughout the call. Since longer calls have been associated with increased success, this shift in approach may have a dramatic effect on conversions. Second, marketing must be more directed towards older clients, emphasizing concerns such as retirement security and low-risk investing. Older clients tend to value steady returns and may find more attractive financial products that are oriented towards delivering on security instead of risky investments. Additionally, restricting the number of client contacts could also be desirable where a high-quality initial contact would be more effective than excessive follow-ups. Overloading with communication could result in client fatigue or frustration, which ultimately reduces the prospect of a positive response. Instead, banks should focus on making a persuasive initial impression by providing clear information about the worth of the term deposit, solving likely issues, and offering customized information. Finally, banks should refrain from segmenting marketing campaigns purely on the basis of occupation or education level because econometric models consistently find that these variables have relatively little impact on term deposit subscription forecasting. Instead, banks should focus more on variables like call length and customer age, which have proved significantly stronger in terms of predictive power. By shifting marketing budgets to target customers with stronger drivers, banks can develop smarter and more effective campaigns. This tactical step would not only reduce the cost and effort involved in wide, less effective outreach but also enhance the conversion rate. Furthermore, this data-driven strategy can optimize overall functional efficiency, optimize resource allocation, and ultimately lead to more effective and profitable term deposit subscription outcomes.\n\n\nConclusion\nIn all, this study aimed to predict whether or not customers would subscribe to a term deposit using the data obtained from the telemarketing campaign of a Portuguese bank. Through the usage of several machine learning techniques including logistic regression, lasso logistic regression, random forest, and gradient boosting, this project identified significant predictors of the behavior of taking up a term deposit by a client. Across models, call duration was the strongest variable overall, followed by age and then campaign. On the other hand, demographics like occupation type and education level were found to possess relatively little predictive power, and therefore these kinds of demographic information may not be as useful for targeted advertising campaigns.\nProject limitations include temporal and geographical constraints of the dataset. Since the information is based on a sequence of marketing campaigns that were conducted by a Portuguese bank between 2008 and 2010, the findings might not be applicable to other regions, time frames, or economic conditions. Market trends, consumer preferences, and economic conditions can change over time, and thus the inferences made from this information may not be applicable to modern or international campaigns. The other important limitation is the employment of statistical data, which may not be accounting for the overall depth of customer behavior. The emotional tone variables of calls, recent financial transactions, or macroeconomic change are absent from the data set, but these can influence a customer’s decision to take a term deposit. Lastly, model problems also arose, particularly in regard to algorithms like random forest that involve some degree of randomness and, while that randomness is important to avoid overfitting, sometimes created inconsistency in the feature importance or predictions. Additionally, the default variable, in this case particularly the “yes” category, was an extremely rare representation (there were only three instances) and might skew the regressions or make the model’s output less robust. Elimination or rebalancing of this variable in subsequent models would likely lead to more stable and trustworthy predictions. Fixing these constraints could improve model accuracy, stability, and applicability for a broader variety of use cases.\n\n\nReference\nChen, J. (2022, March 20). Term Deposit Definition. Investopedia. https://www.investopedia.com/terms/t/termdeposit.asp\nMoro, S., Cortez, P., & Rita, P. (2014). A data-driven approach to predict the success of bank telemarketing. Decision Support Systems, 62, 22–31. https://repositorio.iscte-iul.pt/bitstream/10071/9499/5/dss_v3.pdf UCI Machine Learning Repository. (n.d.). Archive.ics.uci.edu. https://archive.ics.uci.edu/dataset/222/bank+marketing\nDataset: https://www.kaggle.com/datasets/datasciencedonut/current-nyc-property-sales"
  },
  {
    "objectID": "posts/Ice Cream/HW_2_Jupyter_Notebook_Blogging.html",
    "href": "posts/Ice Cream/HW_2_Jupyter_Notebook_Blogging.html",
    "title": "Settings",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | Exp(95% CI Lower) | 95% CI Upper | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                ci_upper,\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: Exp(95% CI Lower), 8: 95% CI Upper, 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"Exp(95% CI Lower)\", \"95% CI Upper\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_regression_models([model_1, model_2, model_3],\n#                                 [assembler_1, assembler_2, assembler_3],\n#                                 [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")\n\n\nterms = assembler3.getInputCols()\ncoefs = model3.coefficients.toArray()[:len(terms)]\nstdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Filter df_summary if needed\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-30-e88d64471ed3&gt; in &lt;cell line: 0&gt;()\n----&gt; 1 terms = assembler3.getInputCols()\n      2 coefs = model3.coefficients.toArray()[:len(terms)]\n      3 stdErrs = model3.summary.coefficientStandardErrors[:len(terms)]\n      4 \n      5 df_summary = pd.DataFrame({\n\nNameError: name 'assembler3' is not defined\n\n\n\n\n# Create a histogram\ndfpd = DATAFRAME.select([\"Y_VARIABLE\"]).toPandas()\nsns.histplot(dfpd[\"Y_VARIABLE\"], bins=10, kde=True)\n\n\nDataFrame\n\ndf = pd.read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\n\ndf.columns\n\nIndex(['priceper1', 'flavor_descr', 'size1_descr', 'household_id',\n       'household_income', 'household_size', 'usecoup', 'couponper1', 'region',\n       'married', 'race', 'hispanic_origin', 'microwave', 'dishwasher', 'sfh',\n       'internet', 'tvcable'],\n      dtype='object')\n\n\n\ndf2 = df[['priceper1', 'household_income', 'usecoup', 'couponper1', 'household_size']]\n\n\ndf2\n\n\n  \n    \n\n\n\n\n\n\npriceper1\nhousehold_income\nusecoup\ncouponper1\nhousehold_size\n\n\n\n\n0\n3.41\n130000\nTrue\n0.5\n2\n\n\n1\n3.50\n130000\nFalse\n0.0\n2\n\n\n2\n3.50\n130000\nFalse\n0.0\n2\n\n\n3\n3.00\n70000\nFalse\n0.0\n1\n\n\n4\n3.99\n130000\nFalse\n0.0\n3\n\n\n...\n...\n...\n...\n...\n...\n\n\n21969\n3.34\n80000\nFalse\n0.0\n4\n\n\n21970\n1.99\n80000\nFalse\n0.0\n4\n\n\n21971\n4.99\n80000\nFalse\n0.0\n1\n\n\n21972\n3.50\n80000\nFalse\n0.0\n1\n\n\n21973\n3.50\n80000\nFalse\n0.0\n1\n\n\n\n\n21974 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncorr = df2.corr()\n\n\n# Set up the matplotlib figure size\nplt.figure(figsize=(8, 6))\n\n# Generate a heatmap in seaborn:\n# - 'corr' is the correlation matrix\n# - 'annot=True' enables annotations inside the squares with the correlation values\n# - 'cmap=\"coolwarm\"' assigns a color map from cool to warm (blue to red)\n# - 'fmt=\".2f\"' formats the annotations to two decimal places\n# - 'linewidths=.5' adds lines between each cell\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Title of the heatmap\nplt.title('Correlation Heatmap with Varied Correlations')\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\n\nspark = SparkSession.builder.getOrCreate()\ndf2 = spark.createDataFrame(df2)\n\ndtrain, dtest = df2.randomSplit([0.67, 0.33], seed = 1234)\n\n\n# assembling predictors\n\nassembler_predictors = (\n    ['household_income', 'usecoup', 'couponper1', 'household_size']\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"priceper1\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)\n\n# makting regression table\nprint( regression_table(model_1, assembler_1) )\n\n+------------------+--------+------+------------+---------+--------------+--------------+\n| y: priceper1     |   Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+------------------+--------+------+------------+---------+--------------+--------------+\n| household_income | -0.000 | ***  |      0.024 |   0.000 |       -0.048 |        0.048 |\n| usecoup          | -0.381 | ***  |      0.014 |   0.000 |       -0.409 |       -0.353 |\n| couponper1       |  0.375 | ***  |      0.004 |   0.000 |        0.367 |        0.383 |\n| household_size   | -0.030 | ***  |      0.018 |   0.000 |       -0.067 |        0.006 |\n| Intercept        |  3.495 | ***  |      0.000 |         |        3.495 |        3.495 |\n-----------------------------------------------------------------------------------------\n| Observations     | 14,734 |      |            |         |              |              |\n| R²               |  0.052 |      |            |         |              |              |\n| RMSE             |  0.650 |      |            |         |              |              |\n+------------------+--------+------+------------+---------+--------------+--------------+\n\n\n#RMSE\n\n# Create a new column for squared error\ndtest_1 = dtest_1.withColumn(\"error_sq\", pow(col(\"priceper1\") - col(\"prediction\"), 2))\n\n# Calculate RMSE as the square root of the mean squared error\nrmse_val_1 = dtest_1.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n\nprint(f'RMSE_1: {rmse_val_1:.3f}')\n\nRMSE_1: 0.649\n\n\n\nprint(compare_rmse([dtest_1], \"priceper1\"))\n\n+------+-----------+\n|      |   Model 1 |\n+======+===========+\n| RMSE |     0.649 |\n+------+-----------+\n\n\n\n\nResidual Plot\n\nresidual_plot(dtest_1, \"priceper1\", \"Model 1\")\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Create a Pandas DataFrame from model3's summary information\nterms = assembler_1.getInputCols()\ncoefs = model_1.coefficients.toArray()[:len(terms)]\nstdErrs = model_1.summary.coefficientStandardErrors[:len(terms)]\n\ndf_summary = pd.DataFrame({\n    \"term\": terms,\n    \"estimate\": coefs,\n    \"std_error\": stdErrs\n})\n\n# Plot using the DataFrame columns\nplt.errorbar(df_summary[\"term\"], df_summary[\"estimate\"],\n             yerr = 1.96 * df_summary[\"std_error\"], fmt='o', capsize=5)\nplt.xlabel(\"Terms\")\nplt.ylabel(\"Coefficient Estimate\")\nplt.title(\"Coefficient Estimates (Model 2)\")\nplt.axhline(0, color=\"red\", linestyle=\"--\")  # Add horizontal line at 0\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]